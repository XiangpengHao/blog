---
title: "parquet-linter: A better Parquet is Parquet itself"
date: "2026-02-22"
categories: []
description: "Unleash the performance potential of your Parquet files"
authors: ["Xiangpeng Hao"]
toc: true
reference-location: margin
citation-location: margin
format:
  html: 
    highlight-style: tango 
---

::: {.callout-warning appearance="simple" icon=false}
## Acknowledgments

My work is supported by [funding](https://xiangpeng.systems/fund/) from [InfluxData](https://influxdata.com), [Bauplan](https://www.bauplanlabs.com), [SpiralDB](https://spiraldb.com), and the taxpayers of the State of Wisconsin and the federal government. Much appreciation!
:::

Try it out here: [https://github.com/XiangpengHao/parquet-linter](https://github.com/XiangpengHao/parquet-linter)

```bash
cargo install parquet-linter-cli
parquet-linter data.parquet
```



## Parquet is a spec, not an implementation

When we talk about Parquet, we often refer to one of its readers or writers, likely the [C++](https://github.com/apache/arrow/tree/main/cpp/src/parquet), [Rust](https://github.com/apache/arrow-rs/tree/master/parquet), or [DuckDB](https://github.com/duckdb/duckdb/tree/main/extension/parquet) implementation.

When we complain that Parquet is not great for our use case, we're actually complaining that the Parquet file generated by a specific implementation with a specific set of parameters is not good enough.

As a spec, Parquet is flexible; it doesn't really restrict much on how the data is stored, and it allows very fine-grained configuration on almost every aspect of the file format.
There's no single best "Parquet" for all use cases.

**Configuring the right parameters for the right use case is not easy.**
It often requires a deep understanding of the cascading effects of encodings, compression, layouts, data types, and page/row group sizes.
It's likely that no single person on the planet can get it right on the first try.

## We need different kinds of Parquet 

You might think, why not just use the best practices? 
Because it's not easy to define what is "best".

#### Use case 1: long term archival

Probably the most common use case for Parquet is to archive data for long-term storage.
In this use case, we don't care about decoding speed; we want the data to be as compact as possible.
Smaller Parquet files means lower S3 bill.

#### Use case 2: fast query performance

Parquet is the backbone of lakehouses like [Iceberg](https://iceberg.apache.org/), [Delta Lake](https://delta.io/), and [DuckLake](https://ducklake.select/).
In such use cases, we want to maximize query performance. As a recent study shows ([Liquid Cache](https://github.com/XiangpengHao/liquid-cache)), decoding Parquet takes a significant amount of query time. 
To minimize decoding time, we want to use lightweight compression, or even no compression at all (as in [Databricks' local cache](https://docs.databricks.com/en/optimizations/disk-cache.html)).

#### Use case 3: ecosystem compatibility

Parquet is an open-direct-access format; [Spark](https://github.com/apache/spark/tree/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet), [Trino](https://github.com/trinodb/trino/tree/master/lib/trino-parquet), [DuckDB](https://github.com/duckdb/duckdb/tree/main/extension/parquet), [DataFusion](https://github.com/apache/datafusion), etc., all support it.
But not all readers support the same set of Parquet features. <!-- TODO: show parquet compatibility matrix here -->
In this use case, we'd like to use the most conservative set of Parquet features that are supported by all readers.

## Vision: Parquet Linter

Introducing Parquet Linter: [https://github.com/XiangpengHao/parquet-linter](https://github.com/XiangpengHao/parquet-linter), a tool to check compression, decompression, and compatibility issues in your Parquet files.

![Parquet Linter transforms your Parquet file into a better Parquet file](parquet-linter.png){width=80%}

I envision three levels of Parquet Linter.

#### Level 1: pure gain

In this level, Parquet Linter checks absolutely inappropriate configurations and suggests a better configuration. 

For example, if a column's compression ratio is 1.0 using a compression algorithm (this happens surprisingly often!), then we should just remove the compression because it is pure overhead.

#### Level 2: trade-offs guided

In this level, we allow the user to specify the trade-offs they want to prioritize, e.g., choosing compression ratio over decoding speed.
Then Parquet Linter checks the configuration and suggests a better set of configurations that aligns with the user's trade-offs.

#### Level 3: towards intelligence

The Parquet Linter is essentially a model; it takes workload features as input and outputs a set of configurations to store the data. 

Currently, this model is a set of empirical rules, biased by the experience of a poor database PhD student.
Just like what's happening everywhere else in the world, we can replace the set of rules with a model, maybe even a large language model.
I believe this can be a very interesting research direction going forward.

## Does it work?

I picked 7 files (somewhat randomly) from [HuggingFace's datasets](https://huggingface.co/docs/datasets/index) and ran the Parquet Linter on them. 
The cost is defined by `loading_time_ms + file_size_mb` [^1].

Even with just simple heuristics, Parquet Linter can easily achieve 20% cost reduction.
With the collective wisdom of the community, I'm sure we can do much better.

| | File 0 | File 1 | File 2 | File 3 | File 4 | File 5 | File 6 | Total |
|---|---:|---:|---:|---:|---:|---:|---:|---:|
| HuggingFace default | 468.72 | 455.16 | 415.51 | 123.32 | 449.38 | 153.06 | 646.98 | 2712.13 |
| **parquet-linter** | 475.33 (+1%) | 350.27 (-23%) | 339.17 (-18%) | 122.67 (-1%) | 345.03 (-23%) | 113.60 (-26%) | 581.58 (-10%) | 2327.66 (-14%) |

[^1]: `loading_time_ms` is the time taken to load the file into Arrow RecordBatch. 