---
title: "Build your own S3-Select in 400 lines of Rust"
date: "2025-03-23"
categories: []
description: "DataFusion is ALL YOU NEED"
toc: true
format:
  html: 
    highlight-style: tango 
reference-location: margin
citation-location: margin
---

::: {.callout-warning appearance="simple" icon=false}

## Acknowledgement

This blog post was made possible by [funding](https://xiangpeng.systems/fund/) support from:

1.  [InfluxData](https://influxdata.com) 

2.  Taxpayers of the state of Wisconsin and the federal government.

Your support for science is greatly appreciated!
:::

## TL;DR

[S3-Select](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html) allows you to filter S3 data before it is returned to you.
This way, you save network bandwidth and time by not downloading the entire file.

However, AWS killed the feature in 2024.
But you can build your own S3-Select with **all open-source tools and open standards**. 

This blog post guides you to build your own S3-Select in **400 lines of Rust**[^1] with [FDAP stack](https://www.influxdata.com/glossary/fdap-stack/).
Code is available [here](https://github.com/XiangpengHao/build-your-own-s3-select).

::: {.column-margin}
FDAP: Apache Arrow **F**light, Apache **D**ataFusion, Apache **A**rrow, Apache **P**arquet
:::

![Comparing S3 (left) with S3-Select (right). S3-Select evaluates the filter before returning the data to you, saving network bandwidth.](s3-select.jpg)

## Architecture

![The compute server is a full-fledged DataFusion server that evaluates user queries. The storage server contains Parquet files and also embeds a DataFusion instance for filter evaluation. The compute and storage communicate with each other via Arrow Flight.](arch.jpg)

Figure above shows the overall architecture, with a storage server, a compute client, and the Arrow Flight protocol in the middle.

### Compute
The compute (client) is a full-fledged DataFusion node that is responsible for evaluating user queries.
Unlike S3-Select, which requires the user to manually craft the filter expression in SQL, our compute node will **automatically** push down the filter expression down to the storage server. 


### Storage
The storage (server) is responsible for storing Parquet files and evaluating filters.
Again, we use DataFusion to handle efficient filter evaluation, this includes: [efficiently reading Parquet data](../parquet-pushdown/index.qmd), evaluating filters with multi threads, and sending the data back to the compute node in a streaming fashion.

<!-- DataFusion is one of the [top-performers](https://datafusion.apache.org/blog/2024/11/18/datafusion-fastest-single-node-parquet-clickbench/) in reading Parquet files. -->

### Communication
The compute and storage communicate with each other via Arrow Flight protocol, a high-performance, open-standard protocol for efficient data exchange.
Our communication is based on open standards, so that any system that speaks Arrow Flight can read data from our storage.
Unlike S3-Select which transmits data in CSV format, the data over network remains columnar, so that the compute node can directly operate on it without heavy deserialization.

## Life of a query 
![The life of a query. The compute node pushes down the filter expression to the storage node. The storage node evaluates the filter and sends the data back to the compute node. The compute node evaluates the rest of the query and sends the result back to the user.](life-of-a-query.jpg){width=50%}

Figure above shows the life of a query:

- On receiving a query from users, the compute node first resolves the table schema from the storage node via Arrow Flight.
- Then, the compute generates a query plan and decides which parts of the query should be evaluated locally, and which parts should be pushed down to the storage node.
- Then the compute node sends the query that needs to be evaluated on the storage node.
- The storage node evaluates the filter and sends the data back to the compute node.
- The compute node evaluates the rest of the query (often the computation heavy part, like aggregations and joins) and sends the result back to the user.

The goal is that only filters are pushed down to the storage node, while the computation heavy parts are evaluated on the compute node.
This avoids overloading the processing power of the storage node.

## Implement Storage Server
Now we have all the theoretical background, let's implement the server.

The storage server is defined as follows:
```rust
struct StorageServer {
    execution_plans: Mutex<HashMap<u64, Arc<dyn ExecutionPlan>>>,
    next_id: atomic::AtomicU64,
    ctx: SessionContext,
}
```

- The `execution_plans` is a map from query id to the execution plan.
- The `next_id` is a counter for generating unique query ids, production servers should use a more robust mechanism like [uuid](https://docs.rs/uuid/latest/uuid/index.html).
- The `ctx` is the DataFusion session context. For simplicity, we use a single context for all queries. 

The `StorageServer` implements the `FlightSqlService` trait, which is the core trait for being a Flight SQL server.
In particular, it has three important methods:

- `get_flight_info_schemas` returns the schema of the table (step 1-2).
- `get_flight_info_statement` returns the exact execution plan for each thread (step 3-4).
- `do_get_fallback` is the method that actually gets the data from the storage (step 5-6).

```rust
#[tonic::async_trait]
impl FlightSqlService for StorageServer {
    type FlightService = StorageServer;

    async fn get_flight_info_schemas(  // <1>
        &self,
        query: CommandGetDbSchemas,
        _request: Request<FlightDescriptor>,
    ) -> Result<Response<FlightInfo>, Status> {
		todo!()
    }


    async fn get_flight_info_statement( // <2>
        &self,
        cmd: CommandStatementQuery,
        _request: Request<FlightDescriptor>,
    ) -> Result<Response<FlightInfo>, Status> {
		todo!()
    }

    async fn do_get_fallback( // <3>
        &self,
        _request: Request<Ticket>,
        message: Any,
    ) -> Result<Response<<Self as FlightService>::DoGetStream>, Status> {
		todo!()
    }
}
```
### Implement get schema 

The `get_flight_info_schemas` method passes in a `CommandGetDbSchemas` object, which stores the table name and url.
We first register the table in the DataFusion context, and then return the schema back to the compute node.

```rust
async fn get_flight_info_schemas(
	&self,
	query: CommandGetDbSchemas,
	_request: Request<FlightDescriptor>,
) -> Result<Response<FlightInfo>, Status> {
	let table_url = query.catalog.unwrap();
	let table_name = query.db_schema_filter_pattern.unwrap();
	_ = self
		.ctx
		.register_parquet(&table_name, table_url, Default::default())
		.await;
	let schema = self.ctx.table_provider(&table_name).await.unwrap().schema();
	let info = FlightInfo::new().try_with_schema(&schema).unwrap();
	Ok(Response::new(info))
}
```

### Implement plan generation

The `get_flight_info_statement` method passes in a `CommandStatementQuery` object, which stores the query string:

- We first parse the SQL into a logical plan, and then optimize it into a physical plan.
- Then we save the physical plan by its id, so that the future pull requests can use this id to get the plan. 
- The physical plan is partitioned so we need to tell the compute to pull each of the partition correspondingly. 
- Finally, we return the partition info and the query id back to the compute node.

```rust
async fn get_flight_info_statement(
	&self,
	cmd: CommandStatementQuery,
	_request: Request<FlightDescriptor>,
) -> Result<Response<FlightInfo>, Status> {
	let query = cmd.query.as_str();
	let (state, logical_plan) = self.ctx.sql(query).await.unwrap().into_parts();
	let plan = state.optimize(&logical_plan).unwrap();
	let physical_plan = state.create_physical_plan(&plan).await.unwrap();
	let partition_count = physical_plan.output_partitioning().partition_count();
	let schema = physical_plan.schema();
	let id = self.next_id.fetch_add(1, atomic::Ordering::Relaxed);
	self.execution_plans
		.lock()
		.unwrap()
		.insert(id, physical_plan);
	let mut info = FlightInfo::new().try_with_schema(&schema).unwrap();
	for partition in 0..partition_count {
		let fetch = FetchResults {
			handle: id,
			partition: partition as u32,
		};
		let buf = fetch.as_any().encode_to_vec().into();
		let ticket = Ticket { ticket: buf };
		let endpoint = FlightEndpoint::new().with_ticket(ticket.clone());
		info = info.with_endpoint(endpoint);
	}
	Ok(Response::new(info))
}
```

### Implement streaming data 

The `do_get_fallback` method is the core method that actually gets the data from the storage, it passes in the query id and partition id, and returns a stream of data.

- It first gets the physical plan from the `execution_plans` map.
- Then it executes the physical plan on the given partition, and returns the result as a stream.

```rust
async fn do_get_fallback(
	&self,
	_request: Request<Ticket>,
	message: Any,
) -> Result<Response<<Self as FlightService>::DoGetStream>, Status> {
	let fetch_results: FetchResults = message.unpack().unwrap().unwrap();
	let plan_lock = self.execution_plans.lock().unwrap();
	let physical_plan = plan_lock.get(&fetch_results.handle).unwrap().clone();
	let stream = physical_plan
		.execute(fetch_results.partition as usize, self.ctx.task_ctx())
		.unwrap()
		.map_err(|e| arrow_flight::error::FlightError::ExternalError(Box::new(e)));
	let encoder = FlightDataEncoderBuilder::new().build(stream);
	let response_stream =
		encoder.map(|result| result.map_err(|e| Status::internal(e.to_string())));
	Ok(Response::new(Box::pin(response_stream)))
}
```

Surprisingly, that is the all we need to implement the storage server[^2], less than 100 lines of code!


## Implement Compute node 

The client has a little more work to do:

1. It has to communicate the storage server to get the data (`FlightExec`).

2. It has to work with the rest of the query plan to produce the final result (`FlightExec`).

3. It has to decide which parts of the query should be sent to the storage server (`FlightTable`).

Here's a diagram showing the relationship between the `FlightTable` and the `FlightExec`:
The `FlightExec` is a [`ExecutionPlan`](https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html) node created by `FlightTable`, which is a [`TableProvider` in DataFusion](https://docs.rs/datafusion/latest/datafusion/catalog/trait.TableProvider.html).

![An example query plan of our system. The `ParquetExec` and the `FilterExec` [^3] are executed on the storage server, while the rest of the plan is executed on the compute node. The two query plans are connected via the `FlightExec` node, which is created by `FlightTable`](query-plan.jpg)


[^3]: Strictly speaking, the `FilterExec` is merged into the `ParquetExec` in the storage server when [filter pushdown](../parquet-pushdown/index.qmd) is enabled.

### Implement `FlightTable`

`FlightTable` implements `TableProvider`, which has two notable methods:

- `scan` is the method that creates the `FlightExec` node. In this method, we need to send the pushdown filters to remote storage server, and create a `FlightExec` node that can pull data from the storage server.
- `supports_filters_pushdown` is the method that decides what filters can be pushed down to the storage server. 

```rust
pub struct FlightTable {
    channel: Channel,
    server: String,
    table_name: TableReference,
    output_schema: SchemaRef,
}

#[async_trait]
impl TableProvider for FlightTable {
    async fn scan(
        &self,
        _state: &dyn Session,
        projection: Option<&Vec<usize>>,
        filters: &[Expr],
        limit: Option<usize>,
    ) -> Result<Arc<dyn ExecutionPlan>> {
		todo!()
    }

    fn supports_filters_pushdown(
        &self,
        filters: &[&Expr],
    ) -> Result<Vec<TableProviderFilterPushDown>> {
       todo!() 
    }

	// ... other trait methods
}
```

Figure below shows the important steps in the `scan` method:

1. Unparses the filters/projections/limit expressions back to SQL and sends it to the storage server for evaluation.
2. Creates the `FlightExec` node that can pull data from the storage server.

![FlightTable unparses the filters/projections/limit back to SQL and sends it to the storage server for evaluation.](unparse.jpg){width=60%}

Here we leverage the fact that DataFusion query planner is able to rewrite and pushdown the filters/projections/limit expressions down to data scanning nodes.
We then uses DataFusion's `unparser` to convert these expressions back to SQL. 
While this sounds quite complex, DataFusion already does the heavy lifting for us.

```rust
async fn scan(
	&self,
	_state: &dyn Session,
	projection: Option<&Vec<usize>>,
	filters: &[Expr],
	limit: Option<usize>,
) -> Result<Arc<dyn ExecutionPlan>> {
	let unparsed_sql = {
		// we don't care about actual source for the purpose of unparsing the sql.
		let empty_table_provider = empty::EmptyTable::new(self.schema().clone());
		let table_source = Arc::new(DefaultTableSource::new(Arc::new(empty_table_provider)));

		let logical_plan = TableScan {
			table_name: self.table_name.clone(),
			source: table_source,
			projection: projection.map(|p| p.to_vec()),
			filters: filters.to_vec(),
			fetch: limit,
			projected_schema: Arc::new(self.schema().as_ref().clone().to_dfschema().unwrap()),
		};
		let unparser = Unparser::new(&PostgreSqlDialect {});
		let unparsed_sql = unparser
			.plan_to_sql(&LogicalPlan::TableScan(logical_plan))
			.unwrap();
		unparsed_sql.to_string()
	};

	println!("SQL send to cache: \n{}", unparsed_sql);

	let mut client = FlightSqlServiceClient::new(self.channel.clone());
	let info = client.execute(unparsed_sql, None).await.unwrap();

	Ok(Arc::new(FlightExec::try_new(
		self.schema.clone(),
		info,
		projection,
		&self.server,
	)?))
}
```

In order for the query plan to pushdown the filters, we need to tell DataFusion whether the filters can be pushed down to the storage server.
Our rule is quite simple: if the filter can be unparsed to SQL, we say it can be pushed down.
```rust
fn supports_filters_pushdown(
	&self,
	filters: &[&Expr],
) -> Result<Vec<TableProviderFilterPushDown>> {
	let filter_push_down: Vec<TableProviderFilterPushDown> = filters
		.iter()
		.map(
			|f| match Unparser::new(&PostgreSqlDialect {}).expr_to_sql(f) {
				Ok(_) => TableProviderFilterPushDown::Exact,
				Err(_) => TableProviderFilterPushDown::Unsupported,
			},
		)
		.collect();
	Ok(filter_push_down)
}
```

### Implement `FlightExec`
`FlightExec` is a [`ExecutionPlan`](https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html) node that can pull data from the storage server.

It in fact is a wrapper around the `FlightStream` that will generate async stream of data.
```rust
pub struct FlightExec {
    server: String,
    partitions: Arc<[FlightPartition]>,
    plan_properties: PlanProperties,
}

impl ExecutionPlan for FlightExec {
    fn execute(
        &self,
        partition: usize,
        _context: Arc<TaskContext>,
    ) -> Result<SendableRecordBatchStream> {
        let future_stream = flight_stream(self.partitions[partition].clone(), self.schema());
        Ok(Box::pin(FlightStream {
            state: FlightStreamState::Init,
            future_stream: Some(Box::pin(future_stream)),
            schema: self.schema(),
        }))
    }

	// ... other trait methods
}
```

`FlightStream` is straightforward, it just uses the `arrow-flight`'s primitives to pull data from the storage server and returns it as a stream.
The exact implementation is quite ugly now, as Rust [doesn't support async iterators yet](https://github.com/rust-lang/rust/issues/79024), which means we need to manually implement the stream state machine.

```rust
struct FlightStream {
    state: FlightStreamState,
    future_stream: Option<BoxFuture<'static, Result<SendableRecordBatchStream>>>,
    schema: SchemaRef,
}

impl Stream for FlightStream {
    type Item = Result<RecordBatch>;

    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        let result: Poll<Option<Result<RecordBatch>>> = loop {
            match &mut self.state {
                FlightStreamState::Init => {
                    self.state = FlightStreamState::GetStream(self.future_stream.take().unwrap());
                    continue;
                }
                FlightStreamState::GetStream(fut) => {
                    let stream = ready!(fut.as_mut().poll(cx)).unwrap();
                    self.state = FlightStreamState::Processing(stream);
                    continue;
                }
                FlightStreamState::Processing(stream) => {
                    let result = stream.as_mut().poll_next(cx);
                    break result;
                }
            }
        };
        match result {
            Poll::Ready(Some(Ok(batch))) => Poll::Ready(Some(Ok(batch))),
            Poll::Ready(None) => Poll::Ready(None),
            Poll::Ready(Some(Err(e))) => {
                panic!("Error reading flight stream: {}", e);
            }
            _ => Poll::Pending,
        }
    }
}
```

That's it! We have implemented the necessary components for the client to work with the storage server.


## Putting it all together
Now we have the server and client, let's put it all together into two binaries.

### Server
The server is quite simple, it just needs to start a `FlightService` server and register the address of the storage server.
```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let addr = "127.0.0.1:50051".parse()?;
    Server::builder()
        .add_service(FlightServiceServer::new(StorageServer::default()))
        .serve(addr)
        .await?;
    Ok(())
}
```

### Client
The client is also straightforward, it enables filter pushdown, registers the table, and run the query.
```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut session_config = SessionConfig::from_env()?;
    session_config
        .options_mut()
        .execution
        .parquet
        .pushdown_filters = true;
    let ctx = Arc::new(SessionContext::new_with_config(session_config));

    let cache_server = "http://localhost:50051";
    let table_name = "aws-edge-locations";
    let table_url = "./aws-edge-locations.parquet";
    let sql = format!(
        "SELECT DISTINCT \"city\" FROM \"{table_name}\" WHERE \"country\" = 'United States'"
    );

    let table = FlightTable::create(cache_server, table_name, table_url).await;
    ctx.register_table(table_name, Arc::new(table))?;
    ctx.sql(&sql).await?.show().await?;
    Ok(())
}
```
### Run

```bash
cargo run --bin server
cargo run --bin client
```

On the client side, you should see the following output:

```	
SQL send to cache: 
SELECT "aws-edge-locations"."city" FROM "aws-edge-locations" WHERE ("aws-edge-locations"."country" = 'United States')
+----------------+
| city           |
+----------------+
| New York       |
| Newark         |
| Miami          |
| Philadelphia   |
| Denver         |
| Houston        |
...
```

Note that only the filter is pushed down, and the aggregation is evaluated on the client side.

Congratulations! You have just built your own S3-Select in just 400 lines of Rust!


## What's next: LiquidCache
Thank you for reading this far! Hopefully by now you have a solid understanding of how to build a push-down enabled storage server.

As you may have notices, this blog post really only scratches the surface of what is possible, we still have a long way to go to build a practical production-ready S3-Select alternative.

Therefore, I am excited to announce the project [LiquidCache](https://github.com/XiangpengHao/liquid-cache), a modern, open-source, open-standards, push-down enabled storage system.
LiquidCache builds on the same foundation as this blog post, but adds many more features (checkout our [paper](https://github.com/XiangpengHao/liquid-cache/blob/main/dev/doc/liquid-cache-vldb.pdf) for more details).

This blog post is the open invitation to you to join me in building LiquidCache!
Feel free to try it out and give any feedback you have!


## Conclusion
Hope you agree that building a S3-Select is quite simple once we find the right building blocks.

However, the challenging and fun part is to find out what this block does that, and to thread all the pieces together to make it work.
As the last piece of the code snippet, here's the include list of the `client.rs` file, we have 30+ lines of code just to import what FDAP stack has to offer!

```rust
use arrow::{
    array::RecordBatch,
    datatypes::{SchemaRef, ToByteSlice},
};
use arrow_flight::{
    FlightClient, FlightEndpoint, FlightInfo, Ticket,
    flight_service_client::FlightServiceClient,
    sql::{CommandGetDbSchemas, client::FlightSqlServiceClient},
};
use datafusion::{
    catalog::{Session, TableProvider},
    common::{ToDFSchema, project_schema},
    datasource::{DefaultTableSource, TableType, empty},
    error::{DataFusionError, Result},
    execution::{RecordBatchStream, SendableRecordBatchStream, TaskContext},
    logical_expr::{LogicalPlan, TableProviderFilterPushDown, TableScan},
    physical_expr::EquivalenceProperties,
    physical_plan::{
        DisplayAs, DisplayFormatType, ExecutionPlan, PlanProperties,
        execution_plan::{Boundedness, EmissionType},
        stream::RecordBatchStreamAdapter,
    },
    prelude::*,
    sql::{
        TableReference,
        unparser::{Unparser, dialect::PostgreSqlDialect},
    },
};
use futures::{Stream, TryStreamExt, future::BoxFuture};
use std::task::{Context, Poll, ready};
use std::{any::Any, pin::Pin, sync::Arc};
use tonic::{async_trait, transport::Channel};
```



[^1]: Without blank lines and comments.
[^2]: Of course, a production server needs way more than that: error handling, logging, authentication, etc. But they are orthogonal to what we are doing here.


