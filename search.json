[
  {
    "objectID": "AGENTS.html",
    "href": "AGENTS.html",
    "title": "Xiangpeng's blog",
    "section": "",
    "text": "Writing style: 1. Readable 2. Explicit"
  },
  {
    "objectID": "posts/working-at-influxdata/index.html",
    "href": "posts/working-at-influxdata/index.html",
    "title": "InfluxData > Google and Microsoft",
    "section": "",
    "text": "I have interned at Google, Microsoft (Research), and InfluxData. The experience at InfluxData has been the most enjoyable, and this blog post explains why.\nSmaller companies are more connected to the real business, more connected within the company, and more willing to connect you to the rest of the world."
  },
  {
    "objectID": "posts/working-at-influxdata/index.html#problem-definition",
    "href": "posts/working-at-influxdata/index.html#problem-definition",
    "title": "InfluxData > Google and Microsoft",
    "section": "Problem definition",
    "text": "Problem definition\nNon-problems\nI have worked on great projects: (1) auto-tuned BigTable, (2) designed a new B-Tree system, and (3) implemented StringView in DataFusion.\nI have met great people: Yixin Luo, Badrish Chandramouli, and Andrew Lamb. They are professional and helpful, and I learned a lot from them.\nThe problem\nThe problem is connectivity: Is the project connected to the real product/customer? Am I connected to the company? Am I connected to the rest of the world?\nBig tech has systematic cultural issues that prevent each individual from being connected, and there’s no easy way to fix it."
  },
  {
    "objectID": "posts/working-at-influxdata/index.html#connect-to-business",
    "href": "posts/working-at-influxdata/index.html#connect-to-business",
    "title": "InfluxData > Google and Microsoft",
    "section": "Connect to business",
    "text": "Connect to business\nI have constantly been reminded to connect my intern project to the InfluxData business – how it improves InfluxDB3.0, what Influx queries it can accelerate, how it compares with the current approaches, etc.\nAndrew expects my project to deploy in production rather than behind a feature gate that no one uses, and he worked very hard to make it happen. Andrew also reminded me to connect my presentation to InfluxDB3.0 and discuss how it benefits the product.\nThis is an important feeling of real – I’m not being paid to work on a toy exploration, not a project that requires five extra teams to reach customers. Instead, it is a project grounded by real systems and real needs, and it will be running as part of a real product.\nBeing real means my time and efforts are respected and valued. Most importantly, it means the investment is sustainable because some kind of return is expected."
  },
  {
    "objectID": "posts/working-at-influxdata/index.html#connect-to-peers",
    "href": "posts/working-at-influxdata/index.html#connect-to-peers",
    "title": "InfluxData > Google and Microsoft",
    "section": "Connect to peers",
    "text": "Connect to peers\nDespite being a remote-only company, InfluxData is highly connected: cross-team collaborations, company-wide acknowledgments for great work, and short turn-around for approval.\nI only learned to appreciate the above after seeing teams fighting for the same project, achievements downplayed due to politics, and bureaucratic processes blocking individual growth.\nConnecting to peers means working with real people, rather than a faceless bureaucratic system. You work towards a shared goal, not a surprise order from high above. When unexpected happens, you know who to talk to, and you can reason the consequences by common sense, not with a rule book or guessing what the boss wants.\nBeing treated as a respected individual, rather than some resource/tool/refillable, is sometimes more important than being paid well."
  },
  {
    "objectID": "posts/working-at-influxdata/index.html#connect-to-the-world",
    "href": "posts/working-at-influxdata/index.html#connect-to-the-world",
    "title": "InfluxData > Google and Microsoft",
    "section": "Connect to the world",
    "text": "Connect to the world\nBig techs have trust and ego issues. They develop/control their own programming languages, frameworks, platforms, etc. They do not sincerely share with the rest of the world, and they design procedures to prevent individuals from doing so. This means your work is only visible to a small group of people, even if it could have a much broader impact.\nAt InfluxData, I have seen the opposite. Not only is all of my work open source, but the company is actively helping me share it on the company blog, X, and LinkedIn. Andrew even took me to a DataFusion meetup and introduced me to the larger FDAP community. (Meanwhile, Microsoft asks students to pay the travel fee to present company work at conferences)\nOpen source at InfluxData means not simply moving the repo to the public domain but sincerely engaging with the community, cultivating an environment where everyone is welcome to contribute, and making sure the goals and intentions are clear and well-communicated. As an example, Andrew and I have spent an extensive amount of time writing the blog post and trying our best to share what we have learned with the community, while we could have just merged the PR and moved forward."
  },
  {
    "objectID": "posts/working-at-influxdata/index.html#what-prevents-a-connected-culture",
    "href": "posts/working-at-influxdata/index.html#what-prevents-a-connected-culture",
    "title": "InfluxData > Google and Microsoft",
    "section": "What prevents a connected culture?",
    "text": "What prevents a connected culture?\nSmall companies don’t automatically gain a connected culture, but I can tell that InfluxData and Andrew have made deliberate efforts to make it happen.\nLarge companies, on the other hand, have systematic issues that prevent this connected culture. Even though every individual I met at Google and Microsoft was nice and kind, there’s little they can do to fix the problem from the faceless bureaucratic system – you can’t even name a person to blame; it is the procedure.\nOne of my dreams is to create an employee-first company that prioritizes respect for individuals, explores new ways to organize and connect people, and focuses on maximizing their happiness and creativity rather than just pursuing profit.\n(I’d like to thank Xuanwo for encouraging me to finish this blog post!)"
  },
  {
    "objectID": "posts/research-statement/index.html",
    "href": "posts/research-statement/index.html",
    "title": "My research statement",
    "section": "",
    "text": "North star\n\nResearch should connect to real people.\n\n\nImpact definition\nResearch impact is defined by the number of connections between the paper and the users.\nNote that the research paper itself does not produce an impact, as nobody’s life is changed because of reading a paper—we are not philosophers. This means that the research impact is always indirect; we have to apply the research to a system with real users.\nDue to the indirect nature of the research’s impact, some effort must be made to connect the research to users.\nMy take: Researchers should take a holistic approach, seamlessly threading their work from the research paper to individual users.\n\n\nNo-goals\nNo goals help define goals. I’ve made all three types of mistakes, and this is my self-reflection to avoid repeating them.\n We are tempted to work on projects proven to be publishable, e.g., improving an existing published method. However, it is not uncommon to see chains of research papers working on a topic irrelevant to the rest of the world. In other words, the impact of a research paper is not defined by how many papers cite it.\n Researchers have the privilege to go wild and think big. But this privilege can be misused. Often, it is used to fool ourselves. Some research (e.g., physics and math) are/can be far from users, but not computer science – one of applied science. As applied scientists, we must ensure our “science” applies to the real world instead of just intellectual exercise.\nSome research (e.g., theory) in applied science still enjoys greater freedom than others. However, my research topics are outside them.\n This is the most common research mistake. We often believe that researchers should focus on making good ideas, and someone else will go off to make it happen. This is not true. There are way too many good ideas, yet few are impactful.\nMost ideas are just combinations of existing techniques, and it is hard to believe anything fundamental will be discovered in system research.\nMost importantly, a good idea evolves with the system it builds upon. It is a feedback loop that tries, fails, and repeats. And the great idea that comes out of the system always differs from its initial version. It is never the case that the research flow is a single-directional graph with no feedback from downstream users.\n\n\nToday’s problem vs tomorrow’s problem\nBoth types of problems are important and meaningful. I don’t work on tomorrow’s problems (but I agree that someone must work on future problems).\nWhy not work on future problems:\n\nWe have enough of today’s problems to work on.\nIt’s easier to fool ourselves while working on future problems.\n\nDon’t fool ourselves:\n\nWe like tomorrow’s problem because it’s easier to claim novelty – future problems naturally come with novelty.\nWe like tomorrow’s problem because we don’t have to establish user connections. Instead, we use it as an excuse to avoid working on the most important and challenging parts. We are free to propose anything we want, work around any challenges, and eventually disconnect from the rest of the world.\nHow do we avoid imaginary research? How can we ensure today’s solution is not funny when tomorrow’s problem arrives? Why work on it when future researchers will beat today’s solution with much more practical considerations?\n\n\n\nEngineering problem vs research problem\n My take on research vs engineering problems:\n\nAll engineering problems are meaningful as they come from users and practitioners—no one invented them out of thin air.\nAll research problems are publishable by definition.\nMost engineering problems are also research problems, evidenced by the fact that they haven’t been solved — meaning they are challenging, resource-consuming, or require a deep understanding of practical nuances, which are the definitions of research problems.\nResearch problems, without being grounded by engineering problems, are unlikely to be meaningful.\n\n\n\nWhy this statement?\nI probably only need to publish one more paper to graduate, as I have already published two papers in VLDB/SIGMOD. Moreover, my chance to stay in academia is quite low, meaning that this one-more-paper is likely the last paper I drive to publish in my career.\nI have lots of cool ideas, but I have to choose only one. This statement helps me to crystalize my thoughts when deciding what to work on. It would be great if I knew all of these early in my Ph.D., but it is still not too late to write them down.\nI can get lost when struggling to publish a paper or being determined to secure a faculty job. Everyone starts with the intention of doing meaningful research, but practical considerations forced them to do otherwise. If I encounter those situations (again), this statement would remind me of my north star."
  },
  {
    "objectID": "posts/parquet-pushdown/index.html",
    "href": "posts/parquet-pushdown/index.html",
    "title": "Efficient Filter Pushdown in Parquet",
    "section": "",
    "text": "WarningAcknowledgement\n\n\n\nThis work (the PR, this blog post, parquet-viewer, and LiquidCache) was made possible by funding support from:\n\nInfluxData\nTaxpayers of the state of Wisconsin and the federal government.\n\nYour support for science is greatly appreciated!\nIn the previous post, we discussed how DataFusion prunes Parquet files to skip irrelevant files/row_groups (sometimes also pages).\nThis post discusses how Parquet readers skip irrelevant rows while scanning data."
  },
  {
    "objectID": "posts/parquet-pushdown/index.html#why-filter-pushdown-in-parquet",
    "href": "posts/parquet-pushdown/index.html#why-filter-pushdown-in-parquet",
    "title": "Efficient Filter Pushdown in Parquet",
    "section": "Why filter pushdown in Parquet?",
    "text": "Why filter pushdown in Parquet?\nBelow is a query that reads sensor data with filters on date_time and location:\nSELECT val, location \nFROM sensor_data \nWHERE date_time &gt; '2025-03-12' AND location = 'office';\n\n\n\nParquet pruning skips irrelevant files/row_groups, while filter pushdown skips irrelevant rows. Without filter pushdown, all rows from location, val, and date_time columns are decoded before location='office' is evaluated. Filter pushdown is especially useful when the filter is selective, i.e., removes many rows.\n\n\nIn our setup, sensor data is aggregated by date — each day has its own Parquet file. DataFusion prunes the unneeded Parquet files, i.e., 2025-03-10/11.parquet.\nOnce the files to read are located, the current default implementation reads all the projected columns (sensor_id, val, and location) into Arrow RecordBatches, then applies the filters over location to get the final set of rows.\nA better approach is filter pushdown, which evaluates filter conditions first and only decodes data that passes these conditions. In practice, this works by first processing only the filter columns (like location), building a boolean mask of rows that satisfy our conditions, then using this mask to selectively decode only the relevant rows from other columns (sensor_id, val). This eliminates the waste of decoding rows that will be filtered out.\nWhile simple in theory, practical implementations often make performance worse."
  },
  {
    "objectID": "posts/parquet-pushdown/index.html#why-slower",
    "href": "posts/parquet-pushdown/index.html#why-slower",
    "title": "Efficient Filter Pushdown in Parquet",
    "section": "Why slower?",
    "text": "Why slower?\nAt a high level, the Parquet reader first builds a filter mask – essentially a boolean array indicating which rows meet the filter criteria – and then uses this mask to selectively decode only the needed rows from the remaining columns in the projection.\nLet’s dig into details of how filter pushdown is implemented in the current Rust implementation of Parquet readers.\n\n\n\nImplementation of filter pushdown in Rust Parquet readers – the first phase builds the filter mask, the second phase applies the filter mask to the other columns\n\n\nThe filter pushdown has two phases:\n\nBuild the filter mask (steps 1-3)\nApply the filter mask to the other columns (steps 4-7)\n\nWithin each phase, it takes three steps from Parquet to Arrow:\n\nDecompress the Parquet pages using generic decompression algorithms like LZ4, Zstd, etc. (steps 1, 4, 6)\nDecode the page content into Arrow format (steps 2, 5, 7)\nEvaluate the filter over Arrow data (step 3)\n\nIn the figure above, we can see that location is decompressed and decoded twice, first when building the filter mask (steps 1, 2), and second when building the output (steps 4, 5). This happens for all columns that appear both in the filter and output.\nThe table below shows the corresponding CPU time on the ClickBench query 22:\n\n\n\nDecompress\nDecode\nApply filter\nOthers\n\n\n\n\n206 ms\n117 ms\n22 ms\n48 ms\n\n\n\nClearly, decompress/decode operations dominate the time spent. With filter pushdown, we need to decompress/decode three times; but without filter pushdown, we only need to do this twice. This explains why filter pushdown is slower.\n\n\n\n\n\n\nNote\n\n\n\nHighly selective filters may skip the entire page; but as long as we read one row from the page, we need to decompress/decode the entire page."
  },
  {
    "objectID": "posts/parquet-pushdown/index.html#attempt-cache-filter-columns",
    "href": "posts/parquet-pushdown/index.html#attempt-cache-filter-columns",
    "title": "Efficient Filter Pushdown in Parquet",
    "section": "Attempt: cache filter columns",
    "text": "Attempt: cache filter columns\nIntuitively, caching the filter columns and reusing them later could help.\nBut caching consumes prohibitively high memory:\n\nWe need to cache Arrow arrays, which are on average 4x larger than Parquet data.\nWe need to cache the entire column in memory, because in Phase 1 we build filters over the entire column, and only use it in Phase 2.\nThe memory usage is proportional to the number of filter columns, which can be unboundedly high.\n\nWorse, caching filter columns means we need to read partially from Parquet and partially from cache, which is complex to implement and requires a radical change to the current implementation.\n\n\n\n\n\n\nNote\n\n\n\nFeel the complexity: consider building a cache that properly handles nested columns, multiple filters, and filters with multiple columns."
  },
  {
    "objectID": "posts/parquet-pushdown/index.html#real-solution",
    "href": "posts/parquet-pushdown/index.html#real-solution",
    "title": "Efficient Filter Pushdown in Parquet",
    "section": "Real solution",
    "text": "Real solution\nWe need a solution that:\n\nIs simple to implement, i.e., doesn’t require thousands of lines of code.\nIncurs minimal memory overhead.\n\nThis section describes my &lt;700 LOC PR (with lots of comments and tests) that reduces total ClickBench time by 15%, with up to 2x lower latency for some queries, no obvious regression on other queries, and caches at most 2 pages (~2MB) per column in memory.\n\n\n\nNew decoding pipeline, building filter mask and output columns are interleaved in a single pass, allowing us to cache minimal pages for minimal amount of time\n\n\nThe new pipeline interleaves the previous two phases into a single pass, so that:\n\nThe page being decompressed is immediately used to build filter masks and output columns.\nWe cache the decompressed page for minimal time; after one pass (steps 1-6), the cache memory is released for the next pass.\n\nThis allows the cache to only hold 1 page at a time, and to immediately discard the previous page after it’s used, significantly reducing the memory requirement for caching.\n\nWhat pages are cached?\nYou may have noticed that only location is cached, not val, because val is only used for output. More generally, only columns that appear both in the filter and output are cached, and at most 1 page is cached for each such column.\nMore examples:\nSELECT val \nFROM sensor_data \nWHERE date_time &gt; '2025-03-12' AND location = 'office';\nIn this case, we don’t cache any columns, because val is not used for filtering.\nSELECT COUNT(*) \nFROM sensor_data \nWHERE date_time &gt; '2025-03-12' AND location = 'office';\nIn this case, we also don’t cache any columns, because the output projection is empty after query plan optimization.\n\n\nThen why cache 2 pages/column instead of 1?\nThis is another real-world nuance regarding how Parquet layouts the pages.\nParquet by default encodes data using dictionary encoding, which writes a dictionary page as the first page of a column chunk, followed by the keys referencing the dictionary.\nYou can see this in action using parquet-viewer:\n\n\n\nParquet viewer shows the page layout of a column chunk\n\n\nThis means that to decode a page of data, we actually need to reference two pages: the dictionary page and the data page.\nThis is why we cache 2 pages per column: one dictionary page and one data page. The data page slot will move forward as we read the data; but the dictionary page slot always references the first page.\n\n\n\nCached two pages, one for dictionary (pinned), one for data (moves as we read the data)"
  },
  {
    "objectID": "posts/parquet-pushdown/index.html#how-does-it-perform",
    "href": "posts/parquet-pushdown/index.html#how-does-it-perform",
    "title": "Efficient Filter Pushdown in Parquet",
    "section": "How does it perform?",
    "text": "How does it perform?\nHere are my results on ClickBench on my AMD 9900X machine. The total time is reduced by 15%, with Q23 being 2.24x faster, and queries that get slower are likely due to noise.\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Query        ┃ no-pushdown ┃ new-pushdown ┃        Change ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ QQuery 0     │      0.47ms │       0.43ms │ +1.10x faster │\n│ QQuery 1     │     51.10ms │      50.10ms │     no change │\n│ QQuery 2     │     68.23ms │      64.49ms │ +1.06x faster │\n│ QQuery 3     │     90.68ms │      86.73ms │     no change │\n│ QQuery 4     │    458.93ms │     458.59ms │     no change │\n│ QQuery 5     │    522.06ms │     478.50ms │ +1.09x faster │\n│ QQuery 6     │     49.84ms │      49.94ms │     no change │\n│ QQuery 7     │     55.09ms │      55.77ms │     no change │\n│ QQuery 8     │    565.26ms │     556.95ms │     no change │\n│ QQuery 9     │    575.83ms │     575.05ms │     no change │\n│ QQuery 10    │    164.56ms │     178.23ms │  1.08x slower │\n│ QQuery 11    │    177.20ms │     191.32ms │  1.08x slower │\n│ QQuery 12    │    591.05ms │     569.92ms │     no change │\n│ QQuery 13    │    861.06ms │     848.59ms │     no change │\n│ QQuery 14    │    596.20ms │     580.73ms │     no change │\n│ QQuery 15    │    554.96ms │     548.77ms │     no change │\n│ QQuery 16    │   1175.08ms │    1146.07ms │     no change │\n│ QQuery 17    │   1150.45ms │    1121.49ms │     no change │\n│ QQuery 18    │   2634.75ms │    2494.07ms │ +1.06x faster │\n│ QQuery 19    │     90.15ms │      89.24ms │     no change │\n│ QQuery 20    │    620.15ms │     591.67ms │     no change │\n│ QQuery 21    │    782.38ms │     703.15ms │ +1.11x faster │\n│ QQuery 22    │   1927.94ms │    1404.35ms │ +1.37x faster │\n│ QQuery 23    │   8104.11ms │    3610.76ms │ +2.24x faster │\n│ QQuery 24    │    360.79ms │     330.55ms │ +1.09x faster │\n│ QQuery 25    │    290.61ms │     252.54ms │ +1.15x faster │\n│ QQuery 26    │    395.18ms │     362.72ms │ +1.09x faster │\n│ QQuery 27    │    891.76ms │     959.39ms │  1.08x slower │\n│ QQuery 28    │   4059.54ms │    4137.37ms │     no change │\n│ QQuery 29    │    235.88ms │     228.99ms │     no change │\n│ QQuery 30    │    564.22ms │     584.65ms │     no change │\n│ QQuery 31    │    741.20ms │     757.87ms │     no change │\n│ QQuery 32    │   2652.48ms │    2574.19ms │     no change │\n│ QQuery 33    │   2373.71ms │    2327.10ms │     no change │\n│ QQuery 34    │   2391.00ms │    2342.15ms │     no change │\n│ QQuery 35    │    700.79ms │     694.51ms │     no change │\n│ QQuery 36    │    151.51ms │     152.93ms │     no change │\n│ QQuery 37    │    108.18ms │      86.03ms │ +1.26x faster │\n│ QQuery 38    │    114.64ms │     106.22ms │ +1.08x faster │\n│ QQuery 39    │    260.80ms │     239.13ms │ +1.09x faster │\n│ QQuery 40    │     60.74ms │      73.29ms │  1.21x slower │\n│ QQuery 41    │     58.75ms │      67.85ms │  1.15x slower │\n│ QQuery 42    │     65.49ms │      68.11ms │     no change │\n└──────────────┴─────────────┴──────────────┴───────────────┘\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Benchmark Summary           ┃            ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ Total Time (no-pushdown)    │ 38344.79ms │\n│ Total Time (new-pushdown)   │ 32800.50ms │\n│ Average Time (no-pushdown)  │   891.74ms │\n│ Average Time (new-pushdown) │   762.80ms │\n│ Queries Faster              │         13 │\n│ Queries Slower              │          5 │\n│ Queries with No Change      │         25 │\n└─────────────────────────────┴────────────┘"
  },
  {
    "objectID": "posts/parquet-pushdown/index.html#conclusion",
    "href": "posts/parquet-pushdown/index.html#conclusion",
    "title": "Efficient Filter Pushdown in Parquet",
    "section": "Conclusion",
    "text": "Conclusion\nDespite being simple in theory, filter pushdown in Parquet is non-trivial to implement. It requires understanding both the Parquet format and reader implementation details. The challenge lies in efficiently navigating through the dynamics of decoding, filter evaluation, and memory management."
  },
  {
    "objectID": "posts/string-view-datafusion/index.html",
    "href": "posts/string-view-datafusion/index.html",
    "title": "Use StringView to make DataFusion faster",
    "section": "",
    "text": "Editor note: This blog post is adapted from the InfluxData blog post series, make sure to also check out the original posts 1 2.\nNote: Thanks to InfluxData for sponsoring this work as a summer intern project\nThis blog describes our experience implementing StringView in the Rust implementation of Apache Arrow, and integrating it into Apache DataFusion, significantly accelerating string-intensive queries in the ClickBench benchmark by 20%- 200% (Figure 11).\nGetting significant end-to-end performance improvements was non-trivial. Implementing StringView itself was only a fraction of the effort required. Among other things, we had to optimize UTF-8 validation, implement unintuitive compiler optimizations, tune block sizes, and time GC to realize the FDAP ecosystem’s benefit. With other members of the open source community, we were able to overcome performance bottlenecks that could have killed the project. We would like to contribute by explaining the challenges and solutions in more detail so that more of the community can learn from our experience.\nStringView is based on a simple idea: avoid some string copies and accelerate comparisons with inlined prefixes. Like most great ideas, it is “obvious” only after someone describes it clearly. Although simple, straightforward implementation actually slows down performance for almost every query. We must, therefore, apply astute observations and diligent engineering to realize the actual benefits from StringView.\nAlthough this journey was successful, not all research ideas are as lucky. To accelerate the adoption of research into industry, it is valuable to integrate research prototypes with practical systems. Understanding the nuances of real-world systems makes it more likely that research designs will lead to practical system improvements.\nStringView support was released as part of arrow-rs v52.2.0 and DataFusion v41.0.0. You can try it by setting the schema_force_string_view DataFusion configuration option, and we are hard at work with the community to make it the default. We invite everyone to try it out, take advantage of the effort invested so far, and contribute to making it better."
  },
  {
    "objectID": "posts/string-view-datafusion/index.html#section-1-what-is-stringview",
    "href": "posts/string-view-datafusion/index.html#section-1-what-is-stringview",
    "title": "Use StringView to make DataFusion faster",
    "section": "Section 1: What is StringView?",
    "text": "Section 1: What is StringView?\nThe concept of inlined strings with prefixes (called “German Strings” by Andy Pavlo, in homage to TUM, where the Umbra paper that describes them originated) has been used in many recent database systems (Velox, Polars, DuckDB, CedarDB, etc.) and was introduced to Arrow as a new StringViewArray2 type. Arrow’s original StringArray is very memory efficient but less effective for certain operations. StringViewArray accelerates string-intensive operations via prefix inlining and a more flexible and compact string representation.\n\n\n\nFigure 2: Use StringArray and StringViewArray to represent the same string content.\n\n\nA StringViewArray consists of three components:\n\nThe view array\n\nThe buffers\n\nThe buffer pointers (IDs) that map buffer offsets to their physical locations\n\nEach view is 16 bytes long, and its contents differ based on the string’s length:\n\nstring length &lt; 12 bytes: the first four bytes store the string length, and the remaining 12 bytes store the inlined string.\n\nstring length &gt; 12 bytes: the string is stored in a separate buffer. The length is again stored in the first 4 bytes, followed by the buffer id (4 bytes), the buffer offset (4 bytes), and the prefix (first 4 bytes) of the string.\n\nFigure 2 shows an example of the same logical content (left) using StringArray (middle) and StringViewArray (right):\n\nThe first string – “Apache DataFusion” – is 17 bytes long, and both StringArray and StringViewArray store the string’s bytes at the beginning of the buffer. The StringViewArray also inlines the first 4 bytes – “Apac” – in the view.\n\nThe second string, “InfluxDB” is only 8 bytes long, so StringViewArray completely inlines the string content in the view struct while StringArray stores the string in the buffer as well.\n\nThe third string “Arrow Rust Impl” is 15 bytes long and cannot be fully inlined. StringViewArray stores this in the same form as the first string.\n\nThe last string “Apache DataFusion” has the same content as the first string. It’s possible to use StringViewArray to avoid this duplication and reuse the bytes by pointing the view to the previous location.\n\nStringViewArray provides three opportunities for outperforming StringArray:\n\nLess copying via the offset + buffer format\n\nFaster comparisons using the inlined string prefix\n\nReusing repeated string values with the flexible view layout\n\nThe rest of this blog post discusses how to apply these opportunities in real query scenarios to improve performance, what challenges we encountered along the way, and how we solved them."
  },
  {
    "objectID": "posts/string-view-datafusion/index.html#section-2-faster-parquet-loading",
    "href": "posts/string-view-datafusion/index.html#section-2-faster-parquet-loading",
    "title": "Use StringView to make DataFusion faster",
    "section": "Section 2: Faster Parquet Loading",
    "text": "Section 2: Faster Parquet Loading\nApache Parquet is the de facto format for storing large-scale analytical data commonly stored LakeHouse-style, such as Apache Iceberg and Delta Lake. Efficiently loading data from Parquet is thus critical to query performance in many important real-world workloads.\nParquet encodes strings (i.e., byte array) in a slightly different format than required for the original Arrow StringArray. The string length is encoded inline with the actual string data (as shown in Figure 4 left). As mentioned previously, StringArray requires the data buffer to be continuous and compact—the strings have to follow one after another. This requirement means that reading Parquet string data into an Arrow StringArray requires copying and consolidating the string bytes to a new buffer and tracking offsets in a separate array. Copying these strings is often wasteful. Typical queries filter out most data immediately after loading, so most of the copied data is quickly discarded.\nOn the other hand, reading Parquet data as a StringViewArray can re-use the same data buffer as storing the Parquet pages because StringViewArray does not require strings to be contiguous. For example, in Figure 4, the StringViewArray directly references the buffer with the decoded Parquet page. The string “Arrow Rust Impl” is represented by a view with offset 37 and length 15 into that buffer.\n\n\n\nFigure 4: StringViewArray avoids copying by reusing decoded Parquet pages.\n\n\nMini benchmark\nReusing Parquet buffers is great in theory, but how much does saving a copy actually matter? We can run the following benchmark in arrow-rs to find out:\ncargo bench --bench arrow_reader --features=\"arrow test_common experimental\" \"arrow_array_reader/Binary.*Array/plain encoded\"\nOur benchmarking machine shows that loading BinaryViewArray is almost 2x faster than loading BinaryArray (see next section about why this isn’t StringViewArray).\narrow_array_reader/BinaryArray/plain encoded                        time:   [315.86 µs 317.47 µs 319.00 µs]\n\narrow_array_reader/BinaryViewArray/plain encoded\ntime:   [162.08 µs 162.20 µs 162.32 µs]\nYou can read more on this arrow-rs issue: https://github.com/apache/arrow-rs/issues/5904\n\nSection 2.1: From Binary to Strings\nYou may wonder why we reported performance for BinaryViewArray when this post is about StringViewArray. Surprisingly, initially, our implementation to read StringViewArray from Parquet was much slower than StringArray. Why? TLDR: Although reading StringViewArray copied less data, the initial implementation also spent much more time validating UTF-8 (as shown in Figure 5).\nStrings are stored as byte sequences. When reading data from (potentially untrusted) Parquet files, a Parquet decoder must ensure those byte sequences are valid UTF-8 strings, and most programming languages, including Rust, include highly optimized routines for doing so.\n\n\n\nFigure 5: Time to load strings from Parquet. The UTF-8 validation advantage initially eliminates the advantage of reduced copying for StringViewArray.\n\n\nA StringArray can be validated in a single call to the UTF-8 validation function as it has a continuous string buffer. As long as the underlying buffer is UTF-83, all strings in the array must be UTF-8. The Rust parquet reader makes a single function call to validate the entire buffer.\nHowever, validating an arbitrary StringViewArray requires validating each string with a separate call to the validation function, as the underlying buffer may also contain non-string data (for example, the lengths in Parquet pages).\nUTF-8 validation in Rust is highly optimized and favors longer strings (as shown in Figure 6), likely because it leverages SIMD instructions to perform parallel validation. The benefit of a single function call to validate UTF-8 over a function call for each string more than eliminates the advantage of avoiding the copy for StringViewArray.\n\n\n\nFigure 6: UTF-8 validation throughput vs string length—StringArray’s contiguous buffer can be validated much faster than StringViewArray’s buffer.\n\n\nDoes this mean we should only use StringArray? No! Thankfully, there’s a clever way out. The key observation is that in many real-world datasets, 99% of strings are shorter than 128 bytes, meaning the encoded length values are smaller than 128, in which case the length itself is also valid UTF-8 (in fact, it is ASCII).\nThis observation means we can optimize validating UTF-8 strings in Parquet pages by treating the length bytes as part of a single large string as long as the length value is less than 128. Put another way, prior to this optimization, the length bytes act as string boundaries, which require a UTF-8 validation on each string. After this optimization, only those strings with lengths larger than 128 bytes (less than 1% of the strings in the ClickBench dataset) are string boundaries, significantly increasing the UTF-8 validation chunk size and thus improving performance.\nThe actual implementation is only nine lines of Rust (with 30 lines of comments). You can find more details in the related arrow-rs issue: https://github.com/apache/arrow-rs/issues/5995. As expected, with this optimization, loading StringViewArray is almost 2x faster than loading StringArray.\n\n\nSection 2.2: Be Careful About Implicit Copies\nAfter all the work to avoid copying strings when loading from Parquet, performance was still not as good as expected. We tracked the problem to a few implicit data copies that we weren’t aware of, as described in this issue.\nThe copies we eventually identified come from the following innocent-looking line of Rust code, where self.buf is a reference counted pointer that should transform without copying into a buffer for use in StringViewArray.\nlet block_id = output.append_block(self.buf.clone().into());\nHowever, Rust-type coercion rules favored a blanket implementation that did copy data. This implementation is shown in the following code block where the impl&lt;T: AsRef&lt;[u8]&gt;&gt; will accept any type that implements AsRef&lt;[u8]&gt; and copies the data to create a new buffer. To avoid copying, users need to explicitly call from_vec, which consumes the Vec and transforms it into a buffer.\nimpl&lt;T: AsRef&lt;[u8]&gt;&gt; From&lt;T&gt; for Buffer {\n    fn from(p: T) -&gt; Self {\n        // copies data here\n     ...\n    }\n}\nimpl Buffer { \n  pub fn from_vec&lt;T&gt;(data: Vec&lt;T&gt;) -&gt; Self {\n// zero-copy transformation\n...\n  }\n}\nDiagnosing this implicit copy was time-consuming as it relied on subtle Rust language semantics. We needed to track every step of the data flow to ensure every copy was necessary. To help other users and prevent future mistakes, we also removed the implicit API from arrow-rs in favor of an explicit API. Using this approach, we found and fixed several other unintentional copies in the code base—hopefully, the change will help other downstream users avoid unnecessary copies.\n\n\nSection 2.3: Help the Compiler by Giving it More Information\nThe Rust compiler’s automatic optimizations mostly work very well for a wide variety of use cases, but sometimes, it needs additional hints to generate the most efficient code. When profiling the performance of view construction, we found, counterintuitively, that constructing long strings was 10x faster than constructing short strings, which made short strings slower on StringViewArray than on StringArray!\nAs described in Section 1, StringViewArray treats long and short strings differently. Short strings (&lt;12 bytes) directly inline to the view struct, while long strings only inline the first 4 bytes. The code to construct a view looks something like this:\nif len &lt;= 12 {\n   // Construct 16 byte view for short string\n   let mut view_buffer = [0; 16];\n   view_buffer[0..4].copy_from_slice(&len.to_le_bytes());\n   view_buffer[4..4 + data.len()].copy_from_slice(data);\n   ...\n} else {      \n   // Construct 16 byte view for long string\n   ByteView {\n       length: len,\n       prefix: u32::from_le_bytes(data[0..4].try_into().unwrap()),\n       buffer_index: block_id,\n       offset,\n   }\n}\nIt appears that both branches of the code should be fast: they both involve copying at most 16 bytes of data and some memory shift/store operations. How could the branch for short strings be 10x slower?\nLooking at the assembly code using godbolt, we (with help from Ao Li) found the compiler used CPU load instructions to copy the fixed-sized 4 bytes to the view for long strings, but it calls a function, ptr::copy_non_overlapping, to copy the inlined bytes to the view for short strings. The difference is that long strings have a prefix size (4 bytes) known at compile time, so the compiler directly uses efficient CPU instructions. But, since the size of the short string is unknown to the compiler, it has to call the general-purpose function ptr::copy_non_coverlapping. Making a function call is significant unnecessary overhead compared to a CPU copy instruction.\nHowever, we know something the compiler doesn’t know: the short string size is not arbitrary—it must be between 0 and 12 bytes, and we can leverage this information to avoid the function call. Our solution generates 13 copies of the function using generics, one for each of the possible prefix lengths. The code looks as follows, and checking the assembly code, we confirmed there are no calls to ptr::copy_non_overlapping, and only native CPU instructions are used. For more details, see the ticket.\nfn make_inlined_view&lt;const LEN: usize&gt;(data: &[u8]) -&gt; u128 {\n     let mut view_buffer = [0; 16];\n     view_buffer[0..4].copy_from_slice(&(LEN as u32).to_le_bytes());\n     view_buffer[4..4 + LEN].copy_from_slice(&data[..LEN]);\n     u128::from_le_bytes(view_buffer)\n}\npub fn make_view(data: &[u8], block_id: u32, offset: u32) -&gt; u128 {\n     let len = data.len();\n     // generate special code for each of the 13 possible lengths\n     match len {\n         0 =&gt; make_inlined_view::&lt;0&gt;(data),\n         1 =&gt; make_inlined_view::&lt;1&gt;(data),\n         2 =&gt; make_inlined_view::&lt;2&gt;(data),\n         3 =&gt; make_inlined_view::&lt;3&gt;(data),\n         4 =&gt; make_inlined_view::&lt;4&gt;(data),\n         5 =&gt; make_inlined_view::&lt;5&gt;(data),\n         6 =&gt; make_inlined_view::&lt;6&gt;(data),\n         7 =&gt; make_inlined_view::&lt;7&gt;(data),\n         8 =&gt; make_inlined_view::&lt;8&gt;(data),\n         9 =&gt; make_inlined_view::&lt;9&gt;(data),\n         10 =&gt; make_inlined_view::&lt;10&gt;(data),\n         11 =&gt; make_inlined_view::&lt;11&gt;(data),\n         12 =&gt; make_inlined_view::&lt;12&gt;(data),\n         _ =&gt; {\n           // handle long string\n}}}\n\n\nSection 2.4: End-to-End Query Performance\nIn the previous sections, we went out of our way to make sure loading StringViewArray is faster than StringArray. Before going further, we wanted to verify if obsessing about reducing copies and function calls has actually improved end-to-end performance in real-life queries. To do this, we evaluated a ClickBench query (Q20) in DataFusion that counts how many URLs contain the word \"google\":\nSELECT COUNT(*) FROM hits WHERE \"URL\" LIKE '%google%';\nThis is a relatively simple query; most of the time is spent on loading the “URL” column to find matching rows. The query plan looks like this:\nProjection: COUNT(*) [COUNT(*):Int64;N]\n  Aggregate: groupBy=[[]], aggr=[[COUNT(*)]] [COUNT(*):Int64;N]\n    Filter: hits.URL LIKE Utf8(\"%google%\")\n      TableScan: hits \nWe ran the benchmark in the DataFusion repo like this:\ncargo run --profile release-nonlto --bin dfbench -- clickbench --queries-path benchmarks/queries/clickbench/queries.sql --iterations 3 --query 20 --path benchmarks/data/hits.parquet --string-view\nWith StringViewArray we saw a 24% end-to-end performance improvement, as shown in Figure 7. With the --string-view argument, the end-to-end query time is 944.3 ms, 869.6 ms, 861.9 ms (three iterations). Without --string-view, the end-to-end query time is 1186.1 ms, 1126.1 ms, 1138.3 ms.\n\n\n\nFigure 7: StringView reduces end-to-end query time by 24% on ClickBench Q20.\n\n\nWe also double-checked with detailed profiling and verified that the time reduction is indeed due to faster Parquet loading."
  },
  {
    "objectID": "posts/string-view-datafusion/index.html#section-3-faster-string-operations",
    "href": "posts/string-view-datafusion/index.html#section-3-faster-string-operations",
    "title": "Use StringView to make DataFusion faster",
    "section": "Section 3: Faster String Operations",
    "text": "Section 3: Faster String Operations\nWe have discussed the nuances required to accelerate Parquet loading using StringViewArray by reusing buffers and reducing copies. In this second part of the post, we describe the rest of the journey: implementing additional efficient operations for real query processing.\n\nSection 3.1: Faster comparison\nString comparison is ubiquitous; it is the core of cmp, min/max, and like/ilike kernels. StringViewArray is designed to accelerate such comparisons using the inlined prefix—the key observation is that, in many cases, only the first few bytes of the string determine the string comparison results.\nFor example, to compare the strings InfluxDB with Apache DataFusion, we only need to look at the first byte to determine the string ordering or equality. In this case, since A is earlier in the alphabet than I, Apache DataFusion sorts first, and we know the strings are not equal. Despite only needing the first byte, comparing these strings when stored as a StringArray requires two memory accesses: 1) load the string offset and 2) use the offset to locate the string bytes. For low-level operations such as cmp that are invoked millions of times in the very hot paths of queries, avoiding this extra memory access can make a measurable difference in query performance.\nFor StringViewArray, typically, only one memory access is needed to load the view struct. Only if the result can not be determined from the prefix is the second memory access required. For the example above, there is no need for the second access. This technique is very effective in practice: the second access is never necessary for the more than 60% of real-world strings which are shorter than 12 bytes, as they are stored completely in the prefix.\nHowever, functions that operate on strings must be specialized to take advantage of the inlined prefix. In addition to low-level comparison kernels, we implemented a wide range of other StringViewArray operations that cover the functions and operations seen in ClickBench queries. Supporting StringViewArray in all string operations takes quite a bit of effort, and thankfully the Arrow and DataFusion communities are already hard at work doing so (see https://github.com/apache/datafusion/issues/11752 if you want to help out).\n\n\nSection 3.2: Faster take and filter\nAfter a filter operation such as WHERE url &lt;&gt; ‘’ to avoid processing empty urls, DataFusion will often coalesce results to form a new array with only the passing elements. This coalescing ensures the batches are sufficiently sized to benefit from vectorized processing in subsequent steps.\nThe coalescing operation is implemented using the take and filter kernels in arrow-rs. For StringArray, these kernels require copying the string contents to a new buffer without “holes” in between. This copy can be expensive especially when the new array is large.\nHowever, take and filter for StringViewArray can avoid the copy by reusing buffers from the old array. The kernels only need to create a new list of views that point at the same strings within the old buffers. Figure 8 illustrates the difference between the output of both string representations. StringArray creates two new strings at offsets 0-17 and 17-32, while StringViewArray simply points to the original buffer at offsets 0 and 25.\n\n\n\nFigure 8: Zero-copy take/filter for StringViewArray\n\n\n\n\nSection 3.3: When to GC?\nZero-copy take/filter is great for generating large arrays quickly, but it is suboptimal for highly selective filters, where most of the strings are filtered out. When the cardinality drops, StringViewArray buffers become sparse—only a small subset of the bytes in the buffer’s memory are referred to by any view. This leads to excessive memory usage, especially in a filter-then-coalesce scenario. For example, a StringViewArray with 10M strings may only refer to 1M strings after some filter operations; however, due to zero-copy take/filter, the (reused) 10M buffers can not be released/reused.\nTo release unused memory, we implemented a garbage collection (GC) routine to consolidate the data into a new buffer to release the old sparse buffer(s). As the GC operation copies strings, similarly to StringArray, we must be careful about when to call it. If we call GC too early, we cause unnecessary copying, losing much of the benefit of StringViewArray. If we call GC too late, we hold large buffers for too long, increasing memory use and decreasing cache efficiency. The Polars blog on StringView also refers to the challenge presented by garbage collection timing.\narrow-rs implements the GC process, but it is up to users to decide when to call it. We leverage the semantics of the query engine and observed that the CoalseceBatchesExec operator, which merge smaller batches to a larger batch, is often used after the record cardinality is expected to shrink, which aligns perfectly with the scenario of GC in StringViewArray. We, therefore, implemented the GC procedure inside CoalseceBatchesExec[^5],with a heuristic that estimates when the buffers are too sparse.\n\n\n\n\n\nSection 3.4: The art of function inlining: not too much, not too little\nLike string inlining, function inlining is the process of embedding a short function into the caller to avoid the overhead of function calls (caller/callee save). Usually, the Rust compiler does a good job of deciding when to inline. However, it is possible to override its default using the #[inline(always)] directive. In performance-critical code, inlined code allows us to organize large functions into smaller ones without paying the runtime cost of function invocation.\nHowever, function inlining is not always better, as it leads to larger function bodies that are harder for LLVM to optimize (for example, suboptimal register spilling) and risk overflowing the CPU’s instruction cache. We observed several performance regressions where function inlining caused slower performance when implementing the StringViewArray comparison kernels. Careful inspection and tuning of the code was required to aid the compiler in generating efficient code. More details can be found in this PR: https://github.com/apache/arrow-rs/pull/5900.\n\n\nSection 3.5: Buffer size tuning\nStringViewArray permits multiple buffers, which enables a flexible buffer layout and potentially reduces the need to copy data. However, a large number of buffers slows down the performance of other operations. For example, get_array_memory_size() needs to sum the memory size of each buffer, which takes a long time with thousands of small buffers. In certain cases, we found that multiple calls to concat_batches lead to arrays with millions of buffers, which was prohibitively expensive.\nFor example, consider a StringViewArray with the previous default buffer size of 8 KB. With this configuration, holding 4GB of string data requires almost half a million buffers! Larger buffer sizes are needed for larger arrays, but we cannot arbitrarily increase the default buffer size, as small arrays would consume too much memory (most arrays require at least one buffer). Buffer sizing is especially problematic in query processing, as we often need to construct small batches of string arrays, and the sizes are unknown at planning time.\nTo balance the buffer size trade-off, we again leverage the query processing (DataFusion) semantics to decide when to use larger buffers. While coalescing batches, we combine multiple small string arrays and set a smaller buffer size to keep the total memory consumption low. In string aggregation, we aggregate over an entire Datafusion partition, which can generate a large number of strings, so we set a larger buffer size (2MB).\nTo assist situations where the semantics are unknown, we also implemented a classic dynamic exponential buffer size growth strategy, which starts with a small buffer size (8KB) and doubles the size of each new buffer up to 2MB. We implemented this strategy in arrow-rs and enabled it by default so that other users of StringViewArray can also benefit from this optimization. See this issue for more details: https://github.com/apache/arrow-rs/issues/6094.\n\n\nSection 3.6: End-to-end query performance\nWe have made significant progress in optimizing StringViewArray filtering operations. Now, let’s test it in the real world to see how it works!\nLet’s consider ClickBench query 22, which selects multiple string fields (URL, Title, and SearchPhase) and applies several filters.\nSELECT \n  \"SearchPhrase\", \n  MIN(\"URL\"), MIN(\"Title\"), COUNT(*) AS c, COUNT(DISTINCT \"UserID\") \nFROM hits \nWHERE \n  \"Title\" LIKE '%Google%' AND \n  \"URL\" NOT LIKE '%.google.%' AND \n  \"SearchPhrase\" &lt;&gt; '' \nGROUP BY \"SearchPhrase\" \nORDER BY c DESC \nLIMIT 10;\nWe ran the benchmark using the following command in the DataFusion repo. Again, the --string-view option means we use StringViewArray instead of StringArray.\ncargo run --profile release-nonlto --bin dfbench -- clickbench --queries-path benchmarks/queries/clickbench/queries.sql --iterations 3 --query 22 --path benchmarks/data/hits.parquet --string-view\nTo eliminate the impact of the faster Parquet reading using StringViewArray (see the first part of this blog), Figure 9 plots only the time spent in FilterExec. Without StringViewArray, the filter takes 7.17s; with StringViewArray, the filter only takes 4.86s, a 32% reduction in time. Moreover, we see a 17% improvement in end-to-end query performance.\n\n\n\nFigure 9: StringViewArray reduces the filter time by 32% on ClickBench query 22."
  },
  {
    "objectID": "posts/string-view-datafusion/index.html#section-4-faster-string-aggregation",
    "href": "posts/string-view-datafusion/index.html#section-4-faster-string-aggregation",
    "title": "Use StringView to make DataFusion faster",
    "section": "Section 4: Faster String Aggregation",
    "text": "Section 4: Faster String Aggregation\nSo far, we have discussed how to exploit two StringViewArray features: reduced copy and faster filtering. This section focuses on reusing string bytes to repeat string values.\nAs described in part one of this blog, if two strings have identical values, StringViewArray can use two different views pointing at the same buffer range, thus avoiding repeating the string bytes in the buffer. This makes StringViewArray similar to an Arrow DictionaryArray that stores Strings—both array types work well for strings with only a few distinct values.\nDeduplicating string values can significantly reduce memory consumption in StringViewArray. However, this process is expensive and involves hashing every string and maintaining a hash table, and so it cannot be done by default when creating a StringViewArray. We introduced an opt-in string deduplication mode in arrow-rs for advanced users who know their data has a small number of distinct values, and where the benefits of reduced memory consumption outweigh the additional overhead of array construction.\nOnce again, we leverage DataFusion query semantics to identify StringViewArray with duplicate values, such as aggregation queries with multiple group keys. For example, some ClickBench queries group by two columns:\n\nUserID (an integer with close to 1 M distinct values)\n\nMobilePhoneModel (a string with less than a hundred distinct values)\n\nIn this case, the output row count is count(distinct UserID) * count(distinct MobilePhoneModel), which is 100M. Each string value of MobilePhoneModel is repeated 1M times. With StringViewArray, we can save space by pointing the repeating values to the same underlying buffer.\nFaster string aggregation with StringView is part of a larger project to improve DataFusion aggregation performance. We have a proof of concept implementation with StringView that can improve the multi-column string aggregation by 20%. We would love your help to get it production ready!"
  },
  {
    "objectID": "posts/string-view-datafusion/index.html#section-5-stringview-pitfalls",
    "href": "posts/string-view-datafusion/index.html#section-5-stringview-pitfalls",
    "title": "Use StringView to make DataFusion faster",
    "section": "Section 5: StringView Pitfalls",
    "text": "Section 5: StringView Pitfalls\nMost existing blog posts (including this one) focus on the benefits of using StringViewArray over other string representations such as StringArray. As we have discussed, even though it requires a significant engineering investment to realize, StringViewArray is a major improvement over StringArray in many cases.\nHowever, there are several cases where StringViewArray is slower than StringArray. For completeness, we have listed those instances here:\n\nTiny strings (when strings are shorter than 8 bytes): every element of the StringViewArray consumes at least 16 bytes of memory—the size of the view struct. For an array of tiny strings, StringViewArray consumes more memory than StringArray and thus can cause slower performance due to additional memory pressure on the CPU cache.\n\nMany repeated short strings: Similar to the first point, StringViewArray can be slower and require more memory than a DictionaryArray because 1) it can only reuse the bytes in the buffer when the strings are longer than 12 bytes and 2) 32-bit offsets are always used, even when a smaller size (8 bit or 16 bit) could represent all the distinct values.\n\nFiltering: As we mentioned above, StringViewArrays often consume more memory than the corresponding StringArray, and memory bloat quickly dominates the performance without GC. However, invoking GC also reduces the benefits of less copying so must be carefully tuned."
  },
  {
    "objectID": "posts/string-view-datafusion/index.html#section-6-conclusion-and-takeaways",
    "href": "posts/string-view-datafusion/index.html#section-6-conclusion-and-takeaways",
    "title": "Use StringView to make DataFusion faster",
    "section": "Section 6: Conclusion and Takeaways",
    "text": "Section 6: Conclusion and Takeaways\nIn these two blog posts, we discussed what it takes to implement StringViewArray in arrow-rs and then integrate it into DataFusion. Our evaluations on ClickBench queries show that StringView can improve the performance of string-intensive workloads by up to 2x.\nGiven that DataFusion already performs very well on ClickBench, the level of end-to-end performance improvement using StringViewArray shows the power of this technique and, of course, is a win for DataFusion and the systems that build upon it.\nStringView is a big project that has received tremendous community support. Specifically, we would like to thank @tustvold, @ariesdevil, @RinChanNOWWW, @ClSlaid, @2010YOUY01, @chloro-pn, @a10y, @Kev1n8, @Weijun-H, @PsiACE, @tshauck, and @xinlifoobar for their valuable contributions!\nAs the introduction states, “German Style Strings” is a relatively straightforward research idea that avoid some string copies and accelerates comparisons. However, applying this (great) idea in practice requires a significant investment in careful software engineering. Again, we encourage the research community to continue to help apply research ideas to industrial systems, such as DataFusion, as doing so provides valuable perspectives when evaluating future research questions for the greatest potential impact."
  },
  {
    "objectID": "posts/string-view-datafusion/index.html#footnotes",
    "href": "posts/string-view-datafusion/index.html#footnotes",
    "title": "Use StringView to make DataFusion faster",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBenchmarked with AMD Ryzen 7600x (12 core, 24 threads, 32 MiB L3), WD Black SN770 NVMe SSD (5150MB/4950MB seq RW bandwidth)↩︎\nThere is also a corresponding BinaryViewArray which is similar except that the data is not constrained to be UTF-8 encoded strings.↩︎\nWe also make sure that offsets do not break a UTF-8 code point, which is cheaply validated.↩︎"
  },
  {
    "objectID": "posts/nature-of-string/index.html",
    "href": "posts/nature-of-string/index.html",
    "title": "On the Nature of Strings",
    "section": "",
    "text": "WarningAcknowledgments\n\n\n\nThis work was supported by funding from:\n\nInfluxData, Bauplan, and SpiralDB.\nThe taxpayers of the State of Wisconsin and the federal government.\n\nYour support for science is greatly appreciated!\nStrings are the most common data type we encounter in analytics. When we built LiquidCache – a distributed pushdown cache for DataFusion – we discovered that the way strings are stored and processed has an outsized impact on performance and memory. This post distills key points from my “On the Nature of Strings” talk at DC Systems (slides here; recording coming soon) and explains how we rethink string storage to make LiquidCache fast and lean."
  },
  {
    "objectID": "posts/nature-of-string/index.html#tldr",
    "href": "posts/nature-of-string/index.html#tldr",
    "title": "On the Nature of Strings",
    "section": "TL;DR",
    "text": "TL;DR\n\nWe analyzed the PublicBI dataset (46 tables, 710 queries, 386 GB of data) and found that strings are the most important data type in the dataset: 61 % of bytes are strings, and around 40 % of projected columns are string columns.\nThere are four types of strings, categorized by distinct ratio and average length: \nDictionary encoding is crucial for storing string data: \nDifferent trade-offs for representing variable-length strings: \nInlined prefixes are critical for efficient comparisons: \nNull handling is essential: \nCompression is important:"
  },
  {
    "objectID": "posts/nature-of-string/index.html#conclusion",
    "href": "posts/nature-of-string/index.html#conclusion",
    "title": "On the Nature of Strings",
    "section": "Conclusion",
    "text": "Conclusion\n\nStrings are the most important data type in analytics, and we need to take them seriously.\nStrings have many subtypes and should be treated differently.\nIf you’re not sure, try LiquidCache – it has a state-of-the-art string-handling pipeline and delivers best-in-class string performance with easy integration."
  },
  {
    "objectID": "posts/parquet-to-arrow/index.html",
    "href": "posts/parquet-to-arrow/index.html",
    "title": "Parquet pruning in DataFusion",
    "section": "",
    "text": "Note: special thanks to InfluxData for funding this blog post.\n\nApache Parquet has become the industry standard for storing columnar data, and reading Parquet efficiently – especially from remote storage – is crucial for query performance.\nApache DataFusion implements advanced Parquet pruning techniques to effectively read only the data that matters for a given query.\nAchieving high performance adds complexity. This post provides an overview of the techniques used in DataFusion to selectively read Parquet files.\n\nThe pipeline\nThe diagram below illustrates the Parquet reading pipeline in DataFusion, highlighting how data flows through various pruning stages before being converted to Arrow format:\n\n\nBackground: Parquet file structure\nAs shown in the figure above, each Parquet file has multiple row groups. Each row group contains a set of columns, and each column contains a set of pages.\nPages are the smallest units of data in Parquet files and typically contain compressed and encoded values for a specific column. This hierarchical structure enables efficient columnar access and forms the foundation for the pruning techniques we’ll discuss.\nCheck out Querying Parquet with Millisecond Latency for more details on the Parquet file structure.\n\n\n1. Read metadata\nDataFusion first reads the Parquet metadata to understand the data in the file. Metadata often includes data schema, the exact location of each row group and column chunk, and their corresponding statistics (e.g., min/max values). It also optionally includes page-level stats and Bloom filters. This information is used to prune the file before reading the actual data.\nFetching metadata requires up to two network requests: one to read the footer size from the end of the file, and another to read the footer itself.\nDecoding metadata is generally fast since it only requires parsing a small amount of data. However, for tables with hundreds or thousands of columns, the metadata can become quite large and decoding it can become a bottleneck. This is particularly noticeable when scanning many small files.\nReading metadata is latency-critical, so DataFusion allows users to cache metadata through the ParquetFileReaderFactory trait.\n\n\n2. Prune by projection\nThe simplest yet perhaps most effective pruning is to read only the columns that are needed. This is because queries usually don’t select all columns, e.g., SELECT a FROM table only reads column a. As a columnar format, Parquet allows DataFusion to only read the columns that are needed.\nThis projection pruning happens at the column level and can dramatically reduce I/O when working with wide tables where queries typically access only a small subset of columns.\n\n\n3. Prune by row group stats and Bloom filters\nEach row group has basic stats like min/max values for each column. DataFusion applies the query predicates to these stats to prune row groups, e.g., SELECT * FROM table WHERE a &gt; 10 will only read row groups where a has a max value greater than 10.\nSometimes min/max stats are too simple to prune effectively, so Parquet also supports Bloom filters. DataFusion uses Bloom filters when available.\nBloom filters are particularly effective for equality predicates (WHERE a = 10) and can significantly reduce the number of row groups that need to be read for point queries or queries with highly selective predicates.\n\n\n4. Prune by page stats\nParquet optionally supports page-level stats – similar to row group stats but more fine-grained. DataFusion implements page pruning when the stats are present.\nPage-level pruning provides an additional layer of filtering after row group pruning. It allows DataFusion to skip individual pages within a row group, further reducing the amount of data that needs to be read and decoded.\n\n\n5. Read from storage\nNow we (hopefully) have pruned the Parquet file into small ranges of bytes, i.e., the Access Plan. The last step is to make requests to fetch those bytes and decode them into Arrow RecordBatch.\n\n\n\nBonus: filter pushdown\nSo far we have discussed techniques that prune the Parquet file using only the metadata, i.e., before reading the actual data.\nFilter pushdown, also known as predicate pushdown, is a technique that prunes data during scanning, with filters being generated and applied in the Parquet reader.\n\nUnlike metadata-based pruning which works at the row group or page level, filter pushdown operates at the row level, allowing DataFusion to filter out individual rows that don’t match the query predicates during the decoding process.\nDataFusion implements filter pushdown but has not enabled it by default due to some performance regressions.\nWe are working to remove the remaining performance issues and enable it by default, which we will discuss in the next blog post.\n\n\nConclusion\nDataFusion employs a multi-step approach to Parquet pruning, from column projection to row group stats, page stats, and potentially row-level filtering. Each step may reduce the amount of data to be read and processed, significantly improving query performance."
  },
  {
    "objectID": "posts/what-is-liquid-cache/index.html",
    "href": "posts/what-is-liquid-cache/index.html",
    "title": "What is LiquidCache?",
    "section": "",
    "text": "WarningAcknowledgments\n\n\n\nThis work is supported by funding from: 1.  InfluxData, Bauplan, and SpiralDB. 2.  The taxpayers of the State of Wisconsin and the federal government.\nYour support for science is greatly appreciated!\nLiquidCache is a caching layer that unifies the design goals of compute and storage1.\nIt accelerates query performance without needing to leave Parquet.\nIt addresses this fundamental tension:\nInstead of squeezing the last bits of performance from Parquet2, or trying to create future-proof file formats3, LiquidCache addresses this problem through a new abstraction: the caching layer.\nIt is built on open standards: Parquet for data storage, DataFusion as the query engine, and Arrow Flight for data transfer. This makes LiquidCache highly composable – you can easily integrate it into your existing analytics stack."
  },
  {
    "objectID": "posts/what-is-liquid-cache/index.html#why-liquidcache",
    "href": "posts/what-is-liquid-cache/index.html#why-liquidcache",
    "title": "What is LiquidCache?",
    "section": "Why LiquidCache?",
    "text": "Why LiquidCache?\n\nWe like S3\n\nSimple durability: 11 nines of durability—you never have to worry about data loss.\nSimple scalability: virtually unlimited space and throughput.\n\n\n\nBut S3 is slow and expensive\n\n≈100 ms first‑byte latency plus transfer latency; this quickly adds up when multiple round‑trips are needed to fetch data.4\nStorage, request, and data‑transfer/egress costs; prices have remained largely unchanged for a decade even as underlying hardware has become ~20× cheaper.\n\n4 Exploiting Cloud Object Storage for High-Performance Analytics\n\n\nS3 prices have barely changed for a decade, despite ~20× reductions in underlying hardware costs, credit to Andrew Lamb\n\n\n\n\nLiquidCache: foundation of diskless architectures\n\nCaches are everywhere5: compute‑local caches (e.g., Snowflake/Databricks local NVMe, Spark host caches)6, shared‑nothing caches, and cache services7.\nDLC trilemma: among durability, low latency, and low cost, you can only choose two8. \n\n\n5 The Five-Minute Rule for the Cloud: Caching in Analytics Systems6 DuckDB’s external file cache7 ClickHouse’s distributed cache for S38 The Cloud Storage Triad: Latency, Cost, Durability"
  },
  {
    "objectID": "posts/what-is-liquid-cache/index.html#how-liquidcache-works",
    "href": "posts/what-is-liquid-cache/index.html#how-liquidcache-works",
    "title": "What is LiquidCache?",
    "section": "How LiquidCache Works",
    "text": "How LiquidCache Works\n\nWe like Parquet\n\nAll major query engines support it (DataFusion, Spark, Trino, DuckDB, Snowflake, BigQuery, and more).\nIt is battle‑tested and keeps evolving (e.g., page indexes, new encodings).\nIt is under open, stable governance (Apache Software Foundation), so your data is in good hands.\n\n\n\nBut sometimes we want more aggressive performance\n\nThere are better encodings and compression schemes out there.\nParquet is critical data infrastructure: it evolves cautiously to keep your data safe and stable—it can’t try new research today and abandon your data tomorrow.\n\n\n\nLiquidCache: cache-only, pushdown-optimized data representation\n\nLiquidCache uses state‑of‑the‑art encodings and compression chosen by the workload.9\nLiquid data is invisible to the rest of the ecosystem: it is cache‑only. This means it can freely change its layout, adding or removing encodings without breaking any user code.\nLiquidCache transparently, progressively, and selectively transcodes Parquet data to the liquid format.\nLiquid data is designed for efficient pushdown to save both compute and network resources.\n\n9 The liquid format is heavily inspired by Vortex. We plan to support a Vortex backend in the future.Without any changes to Parquet, LiquidCache takes care of the performance optimizations."
  },
  {
    "objectID": "posts/what-is-liquid-cache/index.html#conclusions",
    "href": "posts/what-is-liquid-cache/index.html#conclusions",
    "title": "What is LiquidCache?",
    "section": "Conclusions",
    "text": "Conclusions\nLiquidCache is the one‑stop shop for diskless, serverless, and pushdown‑native analytics.\nIt is built on open standards (Parquet, Arrow Flight, DataFusion) for easy integration and stable governance.\nLiquidCache caches Parquet as liquid data, which is ultra-optimized for compute pushdown, compressed execution, modern storage, and network‑efficient data transfer.\n\nWho are we?\n\nLiquidCache started as a research project led by Xiangpeng Hao at UW‑Madison ADSL.\nIt was made possible by a research gift from InfluxData. One year later, SpiralDB and Bauplan also joined the journey.10\nLiquidCache will remain a public‑benefit project in appreciation of the support from taxpayers, research gifts, and the open‑source community.\n\n\n\n10 Support our research here!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Database/storage systems",
    "section": "",
    "text": "Stop building systems for agents\n\n\nBuild agent systems for human.\n\n\n\n\n\nFeb 2, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is LiquidCache?\n\n\nA caching layer to unify compute and storage.\n\n\n\n\n\nNov 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Nature of Strings\n\n\nStrings are the most important data type in analytics, and we need to take them seriously.\n\n\n\n\n\nAug 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuild your own S3-Select in 400 lines of Rust\n\n\nDataFusion is ALL YOU NEED\n\n\n\n\n\nMar 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Filter Pushdown in Parquet\n\n\nHow to implement efficient filter pushdown in Parquet readers and why it’s challenging in practice.\n\n\n\n\n\nMar 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhere are we now, system researchers?\n\n\nWhat I cannot create, I do not understand.\n\n\n\n\n\nMar 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOnline Parquet Viewer with Rust\n\n\nNo JS, no server, just Rust\n\n\n\n\n\nNov 12, 2024\n\n\nXiangpeng Hao\n\n\n\n\n\n\n\n\n\n\n\n\nCaching in DataFusion\n\n\nDon’t read twice.\n\n\n\n\n\nOct 27, 2024\n\n\nXiangpeng Hao\n\n\n\n\n\n\n\n\n\n\n\n\nParquet pruning in DataFusion\n\n\nRead no more than you need\n\n\n\n\n\nOct 24, 2024\n\n\nXiangpeng Hao\n\n\n\n\n\n\n\n\n\n\n\n\nUse StringView to make DataFusion faster\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nXiangpeng Hao, Andrew Lamb\n\n\n\n\n\n\n\n\n\n\n\n\nMy research statement\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInfluxData &gt; Google and Microsoft\n\n\n\n\n\n\n\n\nAug 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThree types of good academic advisors\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens when you type a SQL in the database\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/good-advisor/index.html",
    "href": "posts/good-advisor/index.html",
    "title": "Three types of good academic advisors",
    "section": "",
    "text": "Innovators, educators, and entrepreneurs.\nInnovator: good at finding the right thing to do. They have a clear vision of what important problems are. They are often smart people who deeply understand the problems they work on. Students learn a lot about their methodologies, visions, and ways of thinking/reasoning. Example: Mike Stonebraker.\nEducator: good at teaching and empowering the students. They empower their student to reach their potential and to find what they are good at. They motivate their students and ignite their passions. Example: Remzi H. Arpaci-Dusseau(likely also Andrea C. Arpaci-Dusseau, but I haven’t worked with her).\nEntrepreneur: good at resource acquisition and allocation. Students have enough resources to reach their potential. They connect people and match the right students with the right resources. Example: Aditya Akella.\nA good advisor doesn’t necessarily have to be in only one category (e.g., advisors above span multiple categories). Still, a good advisor often has to be in at least one of the categories.\nI’m privileged enough to have worked with all three types of good advisors; however, finding a good advisor requires a lot of luck and effort. Reality check: many faculty members (especially in prestigious universities) are not in any of the categories, and among them, most don’t even try to be a good advisor."
  },
  {
    "objectID": "posts/good-advisor/index.html#good-advisors",
    "href": "posts/good-advisor/index.html#good-advisors",
    "title": "Three types of good academic advisors",
    "section": "",
    "text": "Innovators, educators, and entrepreneurs.\nInnovator: good at finding the right thing to do. They have a clear vision of what important problems are. They are often smart people who deeply understand the problems they work on. Students learn a lot about their methodologies, visions, and ways of thinking/reasoning. Example: Mike Stonebraker.\nEducator: good at teaching and empowering the students. They empower their student to reach their potential and to find what they are good at. They motivate their students and ignite their passions. Example: Remzi H. Arpaci-Dusseau(likely also Andrea C. Arpaci-Dusseau, but I haven’t worked with her).\nEntrepreneur: good at resource acquisition and allocation. Students have enough resources to reach their potential. They connect people and match the right students with the right resources. Example: Aditya Akella.\nA good advisor doesn’t necessarily have to be in only one category (e.g., advisors above span multiple categories). Still, a good advisor often has to be in at least one of the categories.\nI’m privileged enough to have worked with all three types of good advisors; however, finding a good advisor requires a lot of luck and effort. Reality check: many faculty members (especially in prestigious universities) are not in any of the categories, and among them, most don’t even try to be a good advisor."
  },
  {
    "objectID": "posts/good-advisor/index.html#signs-of-bad-advisors",
    "href": "posts/good-advisor/index.html#signs-of-bad-advisors",
    "title": "Three types of good academic advisors",
    "section": "Signs of bad advisors",
    "text": "Signs of bad advisors\n\nAll happy families are alike; each unhappy family is unhappy in its own way. – Leo Tolstoy\n\nLike unhappy families, bad advisors are pretty novel in their ways of being bad; we don’t summarize them here. Instead, we list some key symptoms of working with a bad advisor.\n\nLong time-to-recovery, i.e., too long to recover from a meeting with your advisor.\n\nRegular meetings with your advisor can be stressful; often the time you need to prepare many slides, teach your advisor about your work, and try to get some feedback. Often, especially in the early stages of your Ph.D., you don’t have good presentation/communication skills to explain your work to your advisor. Your advisor can get impatient and think you are not progressing or working hard enough.\nThis can be frustrating and demotivating, and this pattern repeats every week. The question is, how long does it take to recover from this meeting? I have experienced that it can take a few hours, or even an entire day, to recover from the frustration and continue to work on my research. However, it does not have to be this way; a good advisor can motivate students and give concrete feedback to help them improve.\n\nNot being treated like a human, but a tool/machine to generate papers.\n\nGraduate students are first humans, then students, and then researchers, but never tools/machines to generate papers.\nMost bad advisors treat their students as leverage to fulfill their own goals. They often have very concrete expectations from their students: publish X papers in Y top-tier conferences/journals and work on Z topics with W skill sets. They evaluate their student by whether they can meet those expectations; even worse, those expectations are often inexplicit and change over time.\nIf students lag behind expectations – which happens all the time – they are often blamed for not working hard enough or not being smart enough. Worse, they will be threatened to delay graduation, withdraw funding, or even be kicked out of the program. Even if students are on track, they are often not appreciated for their hard work and dedication. They are given more work to do, and the cycle repeats.\nGraduate study is a long journey; without being treated like a real human, you feel suffering, lonely, and lost. But you deserve a supportive, fruitful, and enjoyable PhD study.\n\nNot aware of power dynamics, but you are my boss.\n\nMany people dreamed an advisor-advisee relationship would be like a marriage partnership: they respect, trust, and work together to achieve common goals.\nBut that is never the case in reality; advisors pay their students’ salaries, and they can decide when they can graduate. The conversation between an advisor and a student is never equal, and the power dynamics are always there.\nThe real challenge is whether the advisor is aware of these power dynamics and how they use them. When in disagreement, does the student have the real freedom to say no? Does the advisor sincerely feel comfortable to be challenged? Navigating these power dynamics is art for both students and advisors, but it’s on the advisors to practice and ensure students feel safe and respected."
  },
  {
    "objectID": "posts/good-advisor/index.html#self-reflections",
    "href": "posts/good-advisor/index.html#self-reflections",
    "title": "Three types of good academic advisors",
    "section": "Self-reflections",
    "text": "Self-reflections\nIt’s easy to blame bad advisors, but it is hard not to become one.\nI suffered a lot from bad advisors, but I often wonder if I can be better than them. Toxic advisors result from systematic academia problems, and changing the system is hard. That’s why good advisors are rare and worth being recognized and appreciated.\nWhen pressured with tenure, funding, and publication, will I be as nice/patient/motivating/caring as I imagined myself to be?"
  },
  {
    "objectID": "posts/sql-to-results/index.html",
    "href": "posts/sql-to-results/index.html",
    "title": "What happens when you type a SQL in the database",
    "section": "",
    "text": "A database can be complex; it involves almost all aspects (research communities) of computer science: PL (programming language), SE (software engineering), OS (operating system), networking, storage, theory; more recently, NLP (natural language processing), and ML (machine learning). The database community is centered around the people interested in making the database (the product) better instead of pure intellectual/research interests; it is, therefore, a practical and multi-disciplinary field. This makes databases awesome but also hard to learn.\nAs complex as it is, the boundaries of the building blocks within a database are clear after decades of research and real-world operations. The recent (and state-of-the-art) Apache DataFusion project is a good example of building a database using well-defined industry standards like Apache Arrow, and Apache Parquet. Without home-grown solutions for storage and in-memory representation, DataFusion can be comparable or even better than alternatives like DuckDB.\nThis document aims to explain these well-defined boundaries, namely, how query engines (i.e., OLAP) transform a plain SQL query into the results we want, how every step works, and how they are connected.\n\n\n\n\n\nflowchart LR\n   id1[SQL text] --&gt; |SQL parser| id2[SQL statement] \n   id2 --&gt; |Query planner| id3[Logical plan] --&gt; |Query optimizer| id4[Optimized logical plan] --&gt; |Physical planner| id5\n   id5[Physical plan] --&gt; |Execution| id7[Output]\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a blog post I hoped I knew when I was younger.\nI aim to make multi-year efforts to edit and improve it as I learn more about databases. I sometimes dreamed that this post could evolve to be the database equivalent of the OSTEP book (it might be too ambitious, though)."
  },
  {
    "objectID": "posts/sql-to-results/index.html#preface",
    "href": "posts/sql-to-results/index.html#preface",
    "title": "What happens when you type a SQL in the database",
    "section": "",
    "text": "A database can be complex; it involves almost all aspects (research communities) of computer science: PL (programming language), SE (software engineering), OS (operating system), networking, storage, theory; more recently, NLP (natural language processing), and ML (machine learning). The database community is centered around the people interested in making the database (the product) better instead of pure intellectual/research interests; it is, therefore, a practical and multi-disciplinary field. This makes databases awesome but also hard to learn.\nAs complex as it is, the boundaries of the building blocks within a database are clear after decades of research and real-world operations. The recent (and state-of-the-art) Apache DataFusion project is a good example of building a database using well-defined industry standards like Apache Arrow, and Apache Parquet. Without home-grown solutions for storage and in-memory representation, DataFusion can be comparable or even better than alternatives like DuckDB.\nThis document aims to explain these well-defined boundaries, namely, how query engines (i.e., OLAP) transform a plain SQL query into the results we want, how every step works, and how they are connected.\n\n\n\n\n\nflowchart LR\n   id1[SQL text] --&gt; |SQL parser| id2[SQL statement] \n   id2 --&gt; |Query planner| id3[Logical plan] --&gt; |Query optimizer| id4[Optimized logical plan] --&gt; |Physical planner| id5\n   id5[Physical plan] --&gt; |Execution| id7[Output]\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a blog post I hoped I knew when I was younger.\nI aim to make multi-year efforts to edit and improve it as I learn more about databases. I sometimes dreamed that this post could evolve to be the database equivalent of the OSTEP book (it might be too ambitious, though)."
  },
  {
    "objectID": "posts/sql-to-results/index.html#section-1-end-to-end-view",
    "href": "posts/sql-to-results/index.html#section-1-end-to-end-view",
    "title": "What happens when you type a SQL in the database",
    "section": "Section 1: End-To-End View",
    "text": "Section 1: End-To-End View\n\nInput\n\nTable definition\nWe have the following two tables (adapted from TPC-H spec): lineitem and orders. The lineitem defines the the shipment dates, while the order defines order details.\n\n\n\n\n\nerDiagram\n  lineitem {\n      int l_orderkey\n      int l_linenumber\n      date l_shipdate\n      date l_commitdate\n      date l_receiptdate\n      string l_shipmode\n      string l_comment\n  }\n  orders {\n      int o_orderkey\n      date o_orderdate\n      string o_orderpriority\n      string o_clerk\n      string o_comment\n  }\n\n\n\n\n\n\n\n\nSQL query\nLet’s say we have this simple query (adapted from TPC-H query 5), which finds the l_orderkey, l_shipdate, and o_orderdate of orders that were placed in 1994.\nSELECT\n    l_orderkey, l_shipdate, o_orderdate\nFROM\n    orders\nJOIN\n    lineitem ON l_orderkey = o_orderkey\nWHERE\n    o_orderdate &gt;= DATE '1994-01-01'\n    AND o_orderdate &lt; DATE '1995-01-01';\n\n\n\nOutput\nThe query is pretty simple; it joins two tables on the order key and then filters the results based on the order date. If everything goes well, we should get results similar to this:\n+------------+------------+-------------+\n| l_orderkey | l_shipdate | o_orderdate |\n+------------+------------+-------------+\n| 1          | 1994-06-01 | 1994-05-01  |\n+------------+------------+-------------+"
  },
  {
    "objectID": "posts/sql-to-results/index.html#section-2-parsing",
    "href": "posts/sql-to-results/index.html#section-2-parsing",
    "title": "What happens when you type a SQL in the database",
    "section": "Section 2: Parsing",
    "text": "Section 2: Parsing\nI skipped it for now as it is mostly orthogonal to the data system pipelines.\n\nInput\nThe SQL query text.\n\n\nOutput\nStructured statement from the SQL (significantly simplified for brevity):\nfrom: [\n  TableWithJoins {\n    relation: Table {\n      name: ObjectName([\n        Ident {\n          value: \"orders\",\n          quote_style: None,\n        },\n      ]),\n    },\n    joins: [\n      Join {\n        relation: Table {\n          name: ObjectName([\n            Ident {\n              value: \"lineitem\",\n              quote_style: None,\n            },\n          ]),\n        },\n        join_operator: Inner(\n          On(\n            BinaryOp {\n              left: Identifier(\n                Ident {\n                  value: \"l_orderkey\",\n                  quote_style: None,\n                },\n              ),\n              op: Eq,\n              right: Identifier(\n                Ident {\n                  value: \"o_orderkey\",\n                  quote_style: None,\n                },\n              ),\n            },\n          ),\n        ),\n      },\n    ],\n  },\n],\nselection: Some(\n  BinaryOp {\n    left: BinaryOp {\n      left: Identifier(\n        Ident {\n          value: \"o_orderdate\",\n          quote_style: None,\n        },\n      ),\n      op: GtEq,\n      right: TypedString {\n        data_type: Date,\n        value: \"1994-01-01\",\n      },\n    },\n    op: And,\n    right: BinaryOp {\n      left: Identifier(\n        Ident {\n          value: \"o_orderdate\",\n          quote_style: None,\n        },\n      ),\n      op: Lt,\n      right: TypedString {\n        data_type: Date,\n        value: \"1995-01-01\",\n      },\n    },\n  },\n),"
  },
  {
    "objectID": "posts/sql-to-results/index.html#section-3-query-planning",
    "href": "posts/sql-to-results/index.html#section-3-query-planning",
    "title": "What happens when you type a SQL in the database",
    "section": "Section 3: Query Planning",
    "text": "Section 3: Query Planning\n\nInput\nThe query statement from the last step.\n\n\nOutput\nThe logical query plan is something like this:\nProjection: lineitem.l_orderkey, lineitem.l_shipdate, orders.o_orderdate\n  Filter: orders.o_orderdate &gt;= CAST(Utf8(\"1994-01-01\") AS Date32) AND orders.o_orderdate &lt; CAST(Utf8(\"1995-01-01\") AS Date32)\n    Inner Join:  Filter: lineitem.l_orderkey = orders.o_orderkey\n      TableScan: orders\n      TableScan: lineitem\nPlot it as a tree.\n\n\n\n\n\n\n\n\n\n\n2\n\nProjection: lineitem.l_orderkey, lineitem.l_shipdate, orders.o_orderdate\n\n\n\n3\n\nFilter: orders.o_orderdate &gt;= CAST(Utf8(_1994-01-01_) AS Date32) AND orders.o_orderdate &lt; CAST(Utf8(_1995-01-01_) AS Date32)\n\n\n\n2-&gt;3\n\n\n\n\n\n4\n\nInner Join:  Filter: lineitem.l_orderkey = orders.o_orderkey\n\n\n\n3-&gt;4\n\n\n\n\n\n5\n\nTableScan: orders\n\n\n\n4-&gt;5\n\n\n\n\n\n6\n\nTableScan: lineitem\n\n\n\n4-&gt;6\n\n\n\n\n\n\n\n\n\n\nLogical vs physical.\nTodo: describe why we must distinguish between physical and logical plans."
  },
  {
    "objectID": "posts/sql-to-results/index.html#section-4-query-optimizing",
    "href": "posts/sql-to-results/index.html#section-4-query-optimizing",
    "title": "What happens when you type a SQL in the database",
    "section": "Section 4: Query Optimizing",
    "text": "Section 4: Query Optimizing\n\nInput\nThe (unoptimized) logical plan from the last step.\n\n\nOutput\nAn optimized logical plan.\nProjection: lineitem.l_orderkey, lineitem.l_shipdate, orders.o_orderdate\n  Inner Join: orders.o_orderkey = lineitem.l_orderkey\n    Filter: orders.o_orderdate &gt;= Date32(\"8766\") AND orders.o_orderdate &lt; Date32(\"9131\")\n      TableScan: orders projection=[o_orderkey, o_orderdate], partial_filters=[orders.o_orderdate &gt;= Date32(\"8766\"), orders.o_orderdate &lt; Date32(\"9131\")]\n    TableScan: lineitem projection=[l_orderkey, l_shipdate]\n\n\n\n\n\n\n\n\n\n\n2\n\nProjection: lineitem.l_orderkey, lineitem.l_shipdate, orders.o_orderdate\n\n\n\n3\n\nInner Join: orders.o_orderkey = lineitem.l_orderkey\n\n\n\n2-&gt;3\n\n\n\n\n\n4\n\nFilter: orders.o_orderdate &gt;= Date32(_8766_) AND orders.o_orderdate &lt; Date32(_9131_)\n\n\n\n3-&gt;4\n\n\n\n\n\n6\n\nTableScan: lineitem projection=[l_orderkey, l_shipdate]\n\n\n\n3-&gt;6\n\n\n\n\n\n5\n\nTableScan: orders projection=[o_orderkey, o_orderdate], partial_filters=[orders.o_orderdate &gt;= Date32(_8766_), orders.o_orderdate &lt; Date32(_9131_)]\n\n\n\n4-&gt;5\n\n\n\n\n\n\n\n\n\n\nNote the difference between an unoptimized and an optimized plan! The Filter has been pushed down to lower-level nodes. Part of the projection has been embedded in the TableScan."
  },
  {
    "objectID": "posts/sql-to-results/index.html#section-5-physical-planning",
    "href": "posts/sql-to-results/index.html#section-5-physical-planning",
    "title": "What happens when you type a SQL in the database",
    "section": "Section 5: Physical Planning",
    "text": "Section 5: Physical Planning\n\nInput\nA logical plan.\n\n\nOutput\nA physical plan. Unlike logical plans, physical plans are more concrete about what to do; here’s an example:\nPhysical plan:\nProjectionExec: expr=[l_orderkey@1 as l_orderkey, l_shipdate@2 as l_shipdate, o_orderdate@0 as o_orderdate]\n  CoalesceBatchesExec: target_batch_size=8192\n    HashJoinExec: mode=Partitioned, join_type=Inner, on=[(o_orderkey@0, l_orderkey@0)], projection=[o_orderdate@1, l_orderkey@2, l_shipdate@3]\n      CoalesceBatchesExec: target_batch_size=8192\n        RepartitionExec: partitioning=Hash([o_orderkey@0], 8), input_partitions=8\n          CoalesceBatchesExec: target_batch_size=8192\n            FilterExec: o_orderdate@1 &gt;= 8766 AND o_orderdate@1 &lt; 9131\n              RepartitionExec: partitioning=RoundRobinBatch(8), input_partitions=1\n                CsvExec: file_groups={1 group: [[Users/xiangpeng/work/coding/db-ml/bin/example-data/orders.csv]]}, projection=[o_orderkey, o_orderdate], has_header=true\n      CoalesceBatchesExec: target_batch_size=8192\n        RepartitionExec: partitioning=Hash([l_orderkey@0], 8), input_partitions=8\n          RepartitionExec: partitioning=RoundRobinBatch(8), input_partitions=1\n            CsvExec: file_groups={1 group: [[Users/xiangpeng/work/coding/db-ml/bin/example-data/lineitem.csv]]}, projection=[l_orderkey, l_shipdate], has_header=true\nWe can also plot a physical plan to a tree graph:\n\n\n\n\n\n\n\n\n\n\n1\n\nProjectionExec: expr=[l_orderkey@1 as l_orderkey, l_shipdate@2 as l_shipdate, o_orderdate@0 as o_orderdate]\n\n\n\n2\n\nCoalesceBatchesExec: target_batch_size=8192\n\n\n\n1-&gt;2\n\n\n\n\n\n3\n\nHashJoinExec: mode=Partitioned, join_type=Inner, on=[(o_orderkey@0, l_orderkey@0)], projection=[o_orderdate@1, l_orderkey@2, l_shipdate@3]\n\n\n\n2-&gt;3\n\n\n\n\n\n4\n\nCoalesceBatchesExec: target_batch_size=8192\n\n\n\n3-&gt;4\n\n\n\n\n\n10\n\nCoalesceBatchesExec: target_batch_size=8192\n\n\n\n3-&gt;10\n\n\n\n\n\n5\n\nRepartitionExec: partitioning=Hash([o_orderkey@0], 8), input_partitions=8\n\n\n\n4-&gt;5\n\n\n\n\n\n6\n\nCoalesceBatchesExec: target_batch_size=8192\n\n\n\n5-&gt;6\n\n\n\n\n\n7\n\nFilterExec: o_orderdate@1 &gt;= 8766 AND o_orderdate@1 &lt; 9131\n\n\n\n6-&gt;7\n\n\n\n\n\n8\n\nRepartitionExec: partitioning=RoundRobinBatch(8), input_partitions=1\n\n\n\n7-&gt;8\n\n\n\n\n\n9\n\nCsvExec: file_groups={1 group: [[orders.csv]]}, projection=[o_orderkey, o_orderdate], has_header=true\n\n\n\n8-&gt;9\n\n\n\n\n\n11\n\nRepartitionExec: partitioning=Hash([l_orderkey@0], 8), input_partitions=8\n\n\n\n10-&gt;11\n\n\n\n\n\n12\n\nRepartitionExec: partitioning=RoundRobinBatch(8), input_partitions=1\n\n\n\n11-&gt;12\n\n\n\n\n\n13\n\nCsvExec: file_groups={1 group: [[lineitem.csv]]}, projection=[l_orderkey, l_shipdate], has_header=true\n\n\n\n12-&gt;13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that a physical plan has much more details than a logical plan; it contains everything needed to execute the query!\n\n\n(Optional: we often have physical optimizers that optimize on a physical plan. Omitted here for simplicity)"
  },
  {
    "objectID": "posts/sql-to-results/index.html#section-6-query-execution",
    "href": "posts/sql-to-results/index.html#section-6-query-execution",
    "title": "What happens when you type a SQL in the database",
    "section": "Section 6: Query Execution",
    "text": "Section 6: Query Execution\n\nInput\nA physical plan\n\n\nOutput\nThe final output is like this:\n+------------+------------+-------------+\n| l_orderkey | l_shipdate | o_orderdate |\n+------------+------------+-------------+\n| 1          | 1994-06-01 | 1994-05-01  |\n+------------+------------+-------------+\n\n\nExecution order\nThe simplest execution model is pull-based execution, which implements a post-order traversal of the physical plan. For a tree (like blow), we get a traversal order of D -&gt; E -&gt; B -&gt; F -&gt; G -&gt; C -&gt; A: \nApplying our physical graph above, we get an execution order of:\n\nCsvExec (orders.csv)\nRepartitionExec\nFilterExec\nCoalesceBatchesExec\nRepartitionExec\nCoalesceBatchesExec\nCsvExec (lineitem.csv)\nRepartitionExec\nRepartitionExec\nCoalesceBatchesExec\nHashJoinExec\nCoalesceBatchesExec\nProjectionExec\n\nThe RepartitionExec and CoalesceBatchesExec are executors that partition the data for multi-thread processing (based on the Volcano execution style).\nA simplified, single-threaded, no-partitioned execution order would be:\n\n\n\n\n\ngraph LR;\n    e1[\"CsvExec (orders.csv)\"] --&gt; FilterExec\n    FilterExec --&gt; e2 \n    e2[\"CsvExec (lineitem.csv)\"] --&gt; HashJoinExec\n    HashJoinExec --&gt; ProjectionExec\n\n\n\n\n\n\n\n\nReading from disk\nCSV files are row-based, and we read them row by row, it is efficient when we frequently need to read the whole row. However, modern data analytic workloads do not always need to read the whole row; they often only need to read a subset of columns. In our example above, we only need to read l_orderkey, l_shipdate, o_orderdate, o_orderkey from the tables. If using a row-based file format (like CSV), we need to load all columns into memory, which is inefficient. Column-based file formats (like Apache Parquet) can be more efficient in this case.\nSee the Parquet pruning in DataFusion for more details."
  },
  {
    "objectID": "posts/build-s3-select/index.html",
    "href": "posts/build-s3-select/index.html",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "",
    "text": "WarningAcknowledgement\n\n\n\nThis blog post was made possible by funding support from:\n\nInfluxData\nTaxpayers of the state of Wisconsin and the federal government.\n\nYour support for science is greatly appreciated!"
  },
  {
    "objectID": "posts/build-s3-select/index.html#tldr",
    "href": "posts/build-s3-select/index.html#tldr",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "TL;DR",
    "text": "TL;DR\nS3-Select can filter S3 data before sending it back to you, significantly saving network bandwidth and time. Unfortunately, AWS killed this feature in 2024.\nGood news: you can build your own S3-Select with all open-source tools and open standards! This blog post shows you how to do it in 400 lines of Rust1 using the FDAP stack2.\n1 Without blank lines and comments.2 FDAP: Apache Arrow Flight, Apache DataFusion, Apache Arrow, Apache ParquetComplete code is available in this repository.\nLooking for a more complete solution? Check out LiquidCache, a open-source/open-standard, push-down enabled storage system built on the same principles.\n\n\n\nComparing S3 (left) with S3-Select (right). S3-Select evaluates the filter before returning the data to you, saving network bandwidth."
  },
  {
    "objectID": "posts/build-s3-select/index.html#architecture",
    "href": "posts/build-s3-select/index.html#architecture",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "Architecture",
    "text": "Architecture\nAs we can’t change S3, we assume that the data is stored in our own storage system (often as a cache to S3).\nOur architecture consists of three main components: a storage server, a compute client, and the Arrow Flight protocol connecting them.\n\n\n\nThe compute server is a full-fledged DataFusion server that evaluates user queries. The storage server contains Parquet files and also embeds a DataFusion instance for filter evaluation. The compute and storage communicate with each other via Arrow Flight.\n\n\n\nCompute\nThe compute (client) is a full-fledged DataFusion node that automatically pushes down filter expressions to the storage server. Unlike S3-Select which requires manual SQL filter crafting, our system automatically decides what can be evaluated at the storage layer, while handling complex operations (joins, aggregations) in the compute node.\n\n\nStorage\nThe storage (server) stores Parquet3 files and evaluates the filters pushed down from the compute node. We leverage DataFusion’s highly optimized query engine for filter evaluation, which includes efficient Parquet reading with filter pushdown, parallel filter evaluation across multiple threads, and streaming filtered results back to the compute node.\n3 Parquet is the industry standard columnar storage format. CSV/JSON/etc. are also easily supported, but you should really consider using a real format like Parquet.\n\nCommunication\nCommunication between compute and storage happens via the Arrow Flight protocol – a high-performance, open-standard protocol for efficient data exchange. Unlike S3-Select which transmits data in CSV/JSON format, we keep data in columnar format throughout the entire pipeline. This means the compute node can directly operate on the data without heavy deserialization, and any system that speaks Arrow Flight can read from our storage."
  },
  {
    "objectID": "posts/build-s3-select/index.html#life-of-a-query",
    "href": "posts/build-s3-select/index.html#life-of-a-query",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "Life of a query",
    "text": "Life of a query\nLet’s walk through how a query flows through our system:\n\n\n\nThe life of a query. The compute node pushes down the filter expression to the storage node. The storage node evaluates the filter and sends the data back to the compute node. The compute node evaluates the rest of the query and sends the result back to the user.\n\n\n\nSchema Resolution: When a user submits a query, the compute node first resolves the table schema from the storage node via Arrow Flight.\nQuery Planning: The compute node generates a query plan and decides which parts should be evaluated locally versus pushed down to the storage node.\nFilter Pushdown: The compute node sends filter predicates to the storage node for evaluation close to the data.\nFilter Evaluation: The storage node evaluates these filters and streams only the matching data back to the compute node.\nFinal Processing: The compute node handles the computation-heavy parts (aggregations, joins, etc.) and returns the final result to the user.\n\nNote that only filters are pushed down to the storage node, computation-heavy operations happen at the compute layer. This prevents overloading the storage node’s processing capabilities while reducing network traffic."
  },
  {
    "objectID": "posts/build-s3-select/index.html#implementing-the-storage-server",
    "href": "posts/build-s3-select/index.html#implementing-the-storage-server",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "Implementing the Storage Server",
    "text": "Implementing the Storage Server\nNow that we understand the architecture, let’s build the storage server. Surprisingly, we can implement it in less than 100 lines of code thanks to DataFusion!\nOur storage server is defined as:\nstruct StorageServer {\n    execution_plans: Mutex&lt;HashMap&lt;u64, Arc&lt;dyn ExecutionPlan&gt;&gt;&gt;,\n    next_id: atomic::AtomicU64,\n    ctx: SessionContext,\n}\nLet’s break down what each field does:\n\nexecution_plans: Maps query IDs to their execution plans\nnext_id: Generates unique query IDs4\nctx: The DataFusion session context (we use a single context for all queries for simplicity)\n\n4 In production, you should use a proper unique ID generator like uuid.The StorageServer implements the FlightSqlService trait with three key methods:\n#[tonic::async_trait]\nimpl FlightSqlService for StorageServer {\n    type FlightService = StorageServer;\n\n    async fn get_flight_info_schemas(\n        &self,\n        query: CommandGetDbSchemas,\n        _request: Request&lt;FlightDescriptor&gt;,\n    ) -&gt; Result&lt;Response&lt;FlightInfo&gt;, Status&gt; {\n        todo!()\n    }\n\n\n    async fn get_flight_info_statement(\n        &self,\n        cmd: CommandStatementQuery,\n        _request: Request&lt;FlightDescriptor&gt;,\n    ) -&gt; Result&lt;Response&lt;FlightInfo&gt;, Status&gt; {\n        todo!()\n    }\n\n    async fn do_get_fallback(\n        &self,\n        _request: Request&lt;Ticket&gt;,\n        message: Any,\n    ) -&gt; Result&lt;Response&lt;&lt;Self as FlightService&gt;::DoGetStream&gt;, Status&gt; {\n        todo!()\n    }\n}\nLet’s implement each method one by one.\n\n1. Schema Resolution\nFirst, we need to implement get_flight_info_schemas to handle schema retrieval:\nasync fn get_flight_info_schemas(\n    &self,\n    query: CommandGetDbSchemas,\n    _request: Request&lt;FlightDescriptor&gt;,\n) -&gt; Result&lt;Response&lt;FlightInfo&gt;, Status&gt; {\n    let table_url = query.catalog.unwrap();\n    let table_name = query.db_schema_filter_pattern.unwrap();\n    _ = self\n        .ctx\n        .register_parquet(&table_name, table_url, Default::default())\n        .await;\n    let schema = self.ctx.table_provider(&table_name).await.unwrap().schema();\n    let info = FlightInfo::new().try_with_schema(&schema).unwrap();\n    Ok(Response::new(info))\n}\nThis method:\n\nRegisters the Parquet file with DataFusion\nGets the schema from the registered table\nReturns the schema as a FlightInfo response\n\n\n\n2. Query Planning\nNext, we implement get_flight_info_statement to handle query planning:\nasync fn get_flight_info_statement(\n    &self,\n    cmd: CommandStatementQuery,\n    _request: Request&lt;FlightDescriptor&gt;,\n) -&gt; Result&lt;Response&lt;FlightInfo&gt;, Status&gt; {\n    let query = cmd.query.as_str();\n    let (state, logical_plan) = self.ctx.sql(query).await.unwrap().into_parts();\n    let plan = state.optimize(&logical_plan).unwrap();\n    let physical_plan = state.create_physical_plan(&plan).await.unwrap();\n    let partition_count = physical_plan.output_partitioning().partition_count();\n    let schema = physical_plan.schema();\n    let id = self.next_id.fetch_add(1, atomic::Ordering::Relaxed);\n    self.execution_plans\n        .lock()\n        .unwrap()\n        .insert(id, physical_plan);\n    let mut info = FlightInfo::new().try_with_schema(&schema).unwrap();\n    for partition in 0..partition_count {\n        let fetch = FetchResults {\n            handle: id,\n            partition: partition as u32,\n        };\n        let buf = fetch.as_any().encode_to_vec().into();\n        let ticket = Ticket { ticket: buf };\n        let endpoint = FlightEndpoint::new().with_ticket(ticket.clone());\n        info = info.with_endpoint(endpoint);\n    }\n    Ok(Response::new(info))\n}\nThis method:\n\nParses the SQL into a logical plan\nOptimizes the logical plan and converts it to a physical plan\nReturns partition info and query ID to the compute node\n\n\n\n3. Data Streaming\nFinally, we implement do_get_fallback to handle data streaming:\nasync fn do_get_fallback(\n    &self,\n    _request: Request&lt;Ticket&gt;,\n    message: Any,\n) -&gt; Result&lt;Response&lt;&lt;Self as FlightService&gt;::DoGetStream&gt;, Status&gt; {\n    let fetch_results: FetchResults = message.unpack().unwrap().unwrap();\n    let plan_lock = self.execution_plans.lock().unwrap();\n    let physical_plan = plan_lock.get(&fetch_results.handle).unwrap().clone();\n    let stream = physical_plan\n        .execute(fetch_results.partition as usize, self.ctx.task_ctx())\n        .unwrap()\n        .map_err(|e| arrow_flight::error::FlightError::ExternalError(Box::new(e)));\n    let encoder = FlightDataEncoderBuilder::new().build(stream);\n    let response_stream =\n        encoder.map(|result| result.map_err(|e| Status::internal(e.to_string())));\n    Ok(Response::new(Box::pin(response_stream)))\n}\nThis method:\n\nRetrieves the physical plan for that query\nExecutes the plan on the specified partition\nReturns the stream to the compute node\n\nThat’s it for the storage server! Just these three methods are enough to implement the core functionality5.\n5 Of course, a production server needs way more: error handling, logging, authentication, etc. But these concerns are orthogonal to our core implementation."
  },
  {
    "objectID": "posts/build-s3-select/index.html#implementing-the-compute-node",
    "href": "posts/build-s3-select/index.html#implementing-the-compute-node",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "Implementing the Compute Node",
    "text": "Implementing the Compute Node\nThe compute node has more work to do, as it needs to:\n\nCommunicate with the storage server to get data\nDecide which parts of the query to send to storage\nProcess the filtered data to produce the final result\n\nWe’ll implement this through two main components:\n\nFlightTable: Decides what gets pushed down to storage\nFlightExec: Handles the data streaming from storage\n\nHere’s how they fit together in the query plan:\n\n\n\nAn example query plan showing how FlightTable and FlightExec connect. The ParquetExec and FilterExec6 are executed on the storage server, while the rest of the plan runs on the compute node. The two plans are connected via the FlightExec node.\n6 Strictly speaking, when filter pushdown is enabled, the FilterExec is merged into ParquetExec in the storage server.\n\n\nImplementing FlightTable\nLet’s start with FlightTable, which implements DataFusion’s TableProvider trait:\npub struct FlightTable {\n    channel: Channel,\n    server: String,\n    table_name: TableReference,\n    output_schema: SchemaRef,\n}\n\n#[async_trait]\nimpl TableProvider for FlightTable {\n    async fn scan(\n        &self,\n        _state: &dyn Session,\n        projection: Option&lt;&Vec&lt;usize&gt;&gt;,\n        filters: &[Expr],\n        limit: Option&lt;usize&gt;,\n    ) -&gt; Result&lt;Arc&lt;dyn ExecutionPlan&gt;&gt; {\n        todo!()\n    }\n\n    fn supports_filters_pushdown(\n        &self,\n        filters: &[&Expr],\n    ) -&gt; Result&lt;Vec&lt;TableProviderFilterPushDown&gt;&gt; {\n       todo!() \n    }\n\n    // ... other trait methods\n}\nThe FlightTable has two important methods:\n\nscan: Creates a FlightExec node that pulls data from storage\nsupports_filters_pushdown: Tells DataFusion which filters can be pushed down\n\nLet’s implement the scan method first:\n\n\n\nFlightTable unparses filters, projections, and limits back to SQL and sends them to the storage server.\n\n\nasync fn scan(\n    &self,\n    _state: &dyn Session,\n    projection: Option&lt;&Vec&lt;usize&gt;&gt;,\n    filters: &[Expr],\n    limit: Option&lt;usize&gt;,\n) -&gt; Result&lt;Arc&lt;dyn ExecutionPlan&gt;&gt; {\n    let unparsed_sql = {\n        // we don't care about actual source for the purpose of unparsing the sql.\n        let empty_table_provider = empty::EmptyTable::new(self.schema().clone());\n        let table_source = Arc::new(DefaultTableSource::new(Arc::new(empty_table_provider)));\n\n        let logical_plan = TableScan {\n            table_name: self.table_name.clone(),\n            source: table_source,\n            projection: projection.map(|p| p.to_vec()),\n            filters: filters.to_vec(),\n            fetch: limit,\n            projected_schema: Arc::new(self.schema().as_ref().clone().to_dfschema().unwrap()),\n        };\n        let unparser = Unparser::new(&PostgreSqlDialect {});\n        let unparsed_sql = unparser\n            .plan_to_sql(&LogicalPlan::TableScan(logical_plan))\n            .unwrap();\n        unparsed_sql.to_string()\n    };\n\n    println!(\"SQL send to cache: \\n{}\", unparsed_sql);\n\n    let mut client = FlightSqlServiceClient::new(self.channel.clone());\n    let info = client.execute(unparsed_sql, None).await.unwrap();\n\n    Ok(Arc::new(FlightExec::try_new(\n        self.schema.clone(),\n        info,\n        projection,\n        &self.server,\n    )?))\n}\nThe magic happens in the unparsing step7. We leverage DataFusion’s query planner to:\n7 We can also send DataFusion’s physical plan (like Ballista), or even Substrait to the storage server.\nCreate a logical plan containing the filters, projections, and limits\nUnparse this plan back to SQL using DataFusion’s unparser\nSend this SQL to the storage server for evaluation\nCreate a FlightExec node that will stream data from storage\n\nNext, let’s implement the filter pushdown support:\nfn supports_filters_pushdown(\n    &self,\n    filters: &[&Expr],\n) -&gt; Result&lt;Vec&lt;TableProviderFilterPushDown&gt;&gt; {\n    let filter_push_down: Vec&lt;TableProviderFilterPushDown&gt; = filters\n        .iter()\n        .map(\n            |f| match Unparser::new(&PostgreSqlDialect {}).expr_to_sql(f) {\n                Ok(_) =&gt; TableProviderFilterPushDown::Exact,\n                Err(_) =&gt; TableProviderFilterPushDown::Unsupported,\n            },\n        )\n        .collect();\n    Ok(filter_push_down)\n}\nOur rule is simple but effective: if a filter can be unparsed to SQL, we push it down to storage.\n\n\nImplementing FlightExec\nNow let’s implement FlightExec, which is responsible for streaming data from storage:\npub struct FlightExec {\n    server: String,\n    partitions: Arc&lt;[FlightPartition]&gt;,\n    plan_properties: PlanProperties,\n}\n\nimpl ExecutionPlan for FlightExec {\n    fn execute(\n        &self,\n        partition: usize,\n        _context: Arc&lt;TaskContext&gt;,\n    ) -&gt; Result&lt;SendableRecordBatchStream&gt; {\n        let future_stream = flight_stream(self.partitions[partition].clone(), self.schema());\n        Ok(Box::pin(FlightStream {\n            state: FlightStreamState::Init,\n            future_stream: Some(Box::pin(future_stream)),\n            schema: self.schema(),\n        }))\n    }\n\n    // ... other trait methods\n}\nFlightExec is mainly a wrapper around FlightStream, which handles the async streaming of data from storage:\nstruct FlightStream {\n    state: FlightStreamState,\n    future_stream: Option&lt;BoxFuture&lt;'static, Result&lt;SendableRecordBatchStream&gt;&gt;&gt;,\n    schema: SchemaRef,\n}\n\nimpl Stream for FlightStream {\n    type Item = Result&lt;RecordBatch&gt;;\n\n    fn poll_next(mut self: Pin&lt;&mut Self&gt;, cx: &mut Context&lt;'_&gt;) -&gt; Poll&lt;Option&lt;Self::Item&gt;&gt; {\n        let result: Poll&lt;Option&lt;Result&lt;RecordBatch&gt;&gt;&gt; = loop {\n            match &mut self.state {\n                FlightStreamState::Init =&gt; {\n                    self.state = FlightStreamState::GetStream(self.future_stream.take().unwrap());\n                    continue;\n                }\n                FlightStreamState::GetStream(fut) =&gt; {\n                    let stream = ready!(fut.as_mut().poll(cx)).unwrap();\n                    self.state = FlightStreamState::Processing(stream);\n                    continue;\n                }\n                FlightStreamState::Processing(stream) =&gt; {\n                    let result = stream.as_mut().poll_next(cx);\n                    break result;\n                }\n            }\n        };\n        match result {\n            Poll::Ready(Some(Ok(batch))) =&gt; Poll::Ready(Some(Ok(batch))),\n            Poll::Ready(None) =&gt; Poll::Ready(None),\n            Poll::Ready(Some(Err(e))) =&gt; {\n                panic!(\"Error reading flight stream: {}\", e);\n            }\n            _ =&gt; Poll::Pending,\n        }\n    }\n}\nThe implementation is a bit complex due to Rust’s lack of native async iterators, requiring us to implement a state machine manually. However, the core concept is straightforward – it pulls data from storage and returns it as a stream."
  },
  {
    "objectID": "posts/build-s3-select/index.html#putting-it-all-together",
    "href": "posts/build-s3-select/index.html#putting-it-all-together",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nNow let’s assemble our components into a working system!\n\nServer Binary\nThe server binary is simple – it just starts a Flight service with our StorageServer:\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let addr = \"127.0.0.1:50051\".parse()?;\n    Server::builder()\n        .add_service(FlightServiceServer::new(StorageServer::default()))\n        .serve(addr)\n        .await?;\n    Ok(())\n}\n\n\nClient Binary\nThe client binary configures DataFusion, registers our FlightTable, and runs the query:\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let mut session_config = SessionConfig::from_env()?;\n    session_config\n        .options_mut()\n        .execution\n        .parquet\n        .pushdown_filters = true;\n    let ctx = Arc::new(SessionContext::new_with_config(session_config));\n\n    let cache_server = \"http://localhost:50051\";\n    let table_name = \"aws-edge-locations\";\n    let table_url = \"./aws-edge-locations.parquet\";\n    let sql = format!(\n        \"SELECT DISTINCT \\\"city\\\" FROM \\\"{table_name}\\\" WHERE \\\"country\\\" = 'United States'\"\n    );\n\n    let table = FlightTable::create(cache_server, table_name, table_url).await;\n    ctx.register_table(table_name, Arc::new(table))?;\n    ctx.sql(&sql).await?.show().await?;\n    Ok(())\n}\n\n\nRunning the System\nTo run our S3-Select alternative:\ncargo run --bin server\ncargo run --bin client\nYou should see output like:\n```bash\nSQL to run: \n-------\nSELECT DISTINCT \"city\" FROM \"aws-edge-locations\" WHERE \"country\" = 'United States'\n-------\n\nSQL to pushdown: \n-------\nSELECT \"aws-edge-locations\".\"city\" FROM \"aws-edge-locations\" WHERE (\"aws-edge-locations\".\"country\" = 'United States')\n-------\n\n+----------------+\n| city           |\n+----------------+\n| Boston         |\n| Chicago        |\n| Portland       |\n| New York       |\n| Newark         |\n| Detroit        |\n...\nNotice that only the filter (WHERE \"country\" = 'United States') was pushed down to storage, while the aggregation (DISTINCT) was evaluated by the compute node. This is exactly what we wanted!"
  },
  {
    "objectID": "posts/build-s3-select/index.html#whats-next-liquidcache",
    "href": "posts/build-s3-select/index.html#whats-next-liquidcache",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "What’s Next: LiquidCache",
    "text": "What’s Next: LiquidCache\nCongratulations! You’ve built a working S3-Select alternative in just 400 lines of Rust. However, this blog post only scratches the surface of what’s possible with this architecture.\nTo take this concept further, I’m excited to announce LiquidCache – a modern, open-source, push-down enabled storage system built on the same principles. LiquidCache extends what we’ve built here (to 15k loc) with advanced features like:\n\nAdvanced caching strategies\nAdvanced filter evaluation techniques\nEnhanced reliability and error handling\nPerformance optimizations\n\nCheck out our research paper for technical details and consider contributing to the project!"
  },
  {
    "objectID": "posts/build-s3-select/index.html#conclusion",
    "href": "posts/build-s3-select/index.html#conclusion",
    "title": "Build your own S3-Select in 400 lines of Rust",
    "section": "Conclusion",
    "text": "Conclusion\nBuilding a functional S3-Select alternative is surprisingly simple once you leverage the right building blocks. The FDAP stack (Flight, DataFusion, Arrow, Parquet) provides powerful primitives that handle most of the heavy lifting for us.\nThe real challenge – and fun – lies in understanding what these components do and how to thread them together effectively. As demonstrated by the imports alone, we’re standing on the shoulders of giants:\nuse arrow::{\n    array::RecordBatch,\n    datatypes::{SchemaRef, ToByteSlice},\n};\nuse arrow_flight::{\n    FlightClient, FlightEndpoint, FlightInfo, Ticket,\n    flight_service_client::FlightServiceClient,\n    sql::{CommandGetDbSchemas, client::FlightSqlServiceClient},\n};\nuse datafusion::{\n    catalog::{Session, TableProvider},\n    common::{ToDFSchema, project_schema},\n    datasource::{DefaultTableSource, TableType, empty},\n    error::{DataFusionError, Result},\n    execution::{RecordBatchStream, SendableRecordBatchStream, TaskContext},\n    logical_expr::{LogicalPlan, TableProviderFilterPushDown, TableScan},\n    physical_expr::EquivalenceProperties,\n    physical_plan::{\n        DisplayAs, DisplayFormatType, ExecutionPlan, PlanProperties,\n        execution_plan::{Boundedness, EmissionType},\n        stream::RecordBatchStreamAdapter,\n    },\n    prelude::*,\n    sql::{\n        TableReference,\n        unparser::{Unparser, dialect::PostgreSqlDialect},\n    },\n};\nuse futures::{Stream, TryStreamExt, future::BoxFuture};\nuse std::task::{Context, Poll, ready};\nuse std::{any::Any, pin::Pin, sync::Arc};\nuse tonic::{async_trait, transport::Channel};\nThe power of open-source tools and open standards isn’t just a fallback — it’s a superior approach that liberates us from vendor lock-in and service discontinuations. We’ve demonstrated that not only can we rebuild essential services like S3-Select after their commercial versions are discontinued, but we can create more powerful8, customizable, and cost-effective alternatives.\n\n\n8 Azure also has filter pushdown, but they don’t support Parquet files."
  },
  {
    "objectID": "posts/caching-datafusion/index.html",
    "href": "posts/caching-datafusion/index.html",
    "title": "Caching in DataFusion",
    "section": "",
    "text": "📢📢📢 Checkout LiquidCache, an open-source caching solution that reduces latency by 10x for cloud-native DataFusion.\nNote: my research is funded by InfluxData, which made this blog post possible.\nIn the last post, we discussed how DataFusion prunes Parquet files to read only the necessary data. This post explores DataFusion’s caching mechanisms, which help avoid repeating reads to object storage."
  },
  {
    "objectID": "posts/caching-datafusion/index.html#current-state",
    "href": "posts/caching-datafusion/index.html#current-state",
    "title": "Caching in DataFusion",
    "section": "Current state",
    "text": "Current state\nDataFusion provides a flexible and layered caching architecture that enables developers to optimize data access at multiple levels. The caching system is designed to be extensible, allowing custom implementations while providing sensible defaults. Let’s examine the built-in caching mechanisms that make DataFusion efficient.\n\n1. List files cache\nThe list files cache optimizes directory scanning operations in DataFusion. When a ListingTable needs to access files in a directory, it first checks this cache before performing expensive filesystem listing operations.\nThis cache is implemented as a simple but effective HashMap that stores directory paths as keys and lists of file metadata as values. The metadata includes important file information like size, last modified time, and other attributes that would otherwise require filesystem/network calls to retrieve:\nstruct ListFilesCache {\n    cached: HashMap&lt;Path, Vec&lt;ObjectMeta&gt;&gt;,\n}\n\n\n2. File statistics cache\nThe file statistics cache stores important statistics about each file, such as row count and column statistics (min/max values). These statistics are used by DataFusion’s query optimizer to make better execution decisions, like pruning files that cannot contain matching data.\nThe cache avoids having to recompute/re-decode these statistics for each query by storing them in a HashMap that maps file paths to both file metadata and statistics:\nstruct FileStatisticsCache {\n    cached: HashMap&lt;Path, (ObjectMeta, Statistics)&gt;,\n}\n\n\n3. Parquet metadata cache\nGetting Parquet metadata can be costly for two main reasons:\n\nNetwork overhead: It requires up to 2 separate network requests to retrieve the raw metadata bytes - one for the footer offset and another for the actual metadata\nProcessing overhead: Decoding the metadata can be computationally expensive, especially for tables with many columns\n\nTo address these challenges, DataFusion provides the ParquetFileReaderFactory trait. This trait allows developers to implement custom metadata handling strategies. The factory returns an AsyncFileReader that separates metadata access from data page access, enabling different caching and optimization approaches for each:\npub trait AsyncFileReader: Send {\n    fn get_metadata(&mut self) -&gt; BoxFuture&lt;'_, Result&lt;Arc&lt;ParquetMetaData&gt;&gt;&gt;;\n\n    fn get_bytes(&mut self, range: Range&lt;usize&gt;) -&gt; BoxFuture&lt;'_, Result&lt;Bytes&gt;&gt;;\n\n    ... // other methods\n}\n\n\n4. Parquet file range cache\nLastly and most importantly, DataFusion caches the Parquet file ranges that have been read.\nUnder the hood, DataFusion uses object_store to interact with the S3, GCS, or local filesystem. While object_store provides a unified interface for different storage backends, but it does not implement caching by default.\nHowever, DataFusion’s flexible architecture allows developers to implement custom caching strategies by implementing the AsyncFileReader trait. This enables optimizations like:\n\nCaching frequently accessed data ranges in memory\nImplementing tiered caching (e.g., memory + local disk)\nAdding compression to reduce memory usage\nImplementing cache eviction policies based on access patterns\n\nNotably, the AsyncFileReader trait has a get_bytes_ranges method:\npub trait AsyncFileReader: Send {\n    ... // previously mentioned methods \n\n    fn get_byte_ranges(&mut self, ranges: Vec&lt;Range&lt;usize&gt;&gt;) -&gt; BoxFuture&lt;'_, Result&lt;Vec&lt;Bytes&gt;&gt;&gt;;\n}\nThe default implementation of get_byte_ranges simply calls get_bytes sequentially for each range, which can be inefficient when reading many small ranges. This is because each range requires a separate network request, leading to higher latency and costs.\nHowever, developers can implement their own IO coalescing logic to optimize performance. For example, they could:\n\nMerge adjacent or overlapping ranges to reduce the number of requests\nBatch multiple small ranges into a single larger request\nImplement prefetching for ranges likely to be needed soon\n\nThese optimizations can significantly improve read performance and reduce storage costs, especially when working with remote storage like S3 or GCS.\nSo far we have discussed the core caching mechanisms built into DataFusion. These caches work together to optimize different aspects of query execution, from file discovery to data access.\nThe following sections will explore more advanced topics and future directions for DataFusion’s caching capabilities."
  },
  {
    "objectID": "posts/caching-datafusion/index.html#caching-arrow",
    "href": "posts/caching-datafusion/index.html#caching-arrow",
    "title": "Caching in DataFusion",
    "section": "Caching Arrow",
    "text": "Caching Arrow\nArrow is the in-memory columnar format that DataFusion uses to process data efficiently. Before DataFusion can execute any query logic, Parquet data must be decoded into Arrow format. This decoding process involves decompressing the data, converting between data types, and validating constraints - operations that recent research1 has shown can be a performance bottleneck for many analytical workloads.\nOne promising optimization approach is to cache the decoded Arrow data rather than the raw Parquet bytes. This allows us to skip the expensive decoding step on subsequent queries, potentially improving query latency significantly.\nThe figure below compares query latencies between two caching strategies using the ClickBench benchmark suite. The x-axis shows the query ID (0-42) and the y-axis shows query latency in milliseconds (lower is better). For each query, we measure the latency when caching the raw Parquet bytes versus caching the decoded Arrow arrays.\n\n\nTakeaways\n\nCaching Arrow consistently outperforms or matches caching Parquet across all queries.\nThe performance gains vary significantly:\n\nScan-intensive queries (Q20-Q23) show the largest improvements, with up to 3x speedup, since they benefit directly from avoiding Parquet decoding\nAggregation-heavy queries (Q8-Q18) see more modest gains, as their execution time is dominated by computation rather than data access\n\nMemory usage can be a concern - Q23 triggered an out-of-memory error when caching Arrow data, highlighting its excessive memory usage.\n\n\n\nBut at what cost?\nWhile caching Arrow data can significantly improve query performance, it comes with substantial memory overhead, as demonstrated by Q23’s out-of-memory error. The figure below compares memory usage between caching Parquet versus Arrow data across the benchmark queries. Since each query was run independently, the measurements reflect the memory requirements for executing a single query in isolation.\n\n\nTakeaways\n\nParquet achieves roughly 4x-5x compression ratio compared to Arrow’s in-memory format\nThe benefit of caching Arrow data varies, but its cost is consistently 4x-5x higher memory usage.\n\n\n\n\nZoom-in a bit\nNow we take a closer look at Q21 – one of the queries that benefit a lot from caching Arrow:\nSELECT \"SearchPhrase\", MIN(\"URL\"), COUNT(*) AS c \nFROM hits \nWHERE \"URL\" LIKE '%google%' \n      AND \"SearchPhrase\" &lt;&gt; '' \nGROUP BY \"SearchPhrase\" \nORDER BY c \nDESC LIMIT 10;\nThe query scans two string columns (“URL” and “SearchPhrase”) and applies a filter on them.\nLet’s now compare their query time and memory usage – we got roughly 3x speedup by using 4x more memory, not bad! \n\n\nHow to cache Arrow?\nThe figure below builds on the architecture from our previous post, adding the new Arrow cache component. The Arrow cache is positioned between the ParquetAccessPlan and the decoded Arrow RecordBatch. When a query requests data:\n\nFirst, we check if the requested RecordBatch exists in the cache\nIf found, we can skip both fetching and decoding the Parquet data by pruning the AccessPlan\nIf not found, we fetch and decode the Parquet data as normal, then insert the resulting Arrow RecordBatch into the cache for future use\n\n\nAlthough the architecture is simple, implementing it faces several challenges:\n\nHow to map Parquet byte ranges to the corresponding Arrow RecordBatches\nGranularity/shape of caching – column-level vs batch-level caching.\nHow to efficiently test if the cached ranges contains the requested range? E.g., request range (1024, 2048), but cached ranges has [(0, 2049), (4096, 3072)].\nMemory management – implementing efficient spill-to-disk strategies\n\nWe are actively working on solutions to these challenges as part of our research into high-performance Arrow caching systems. Our goal is to develop practical implementations that can be integrated into production environments. We plan to publish our findings and release the code as open source in the near future. Consider funding my research to support this work."
  },
  {
    "objectID": "posts/caching-datafusion/index.html#standalone-caching-service",
    "href": "posts/caching-datafusion/index.html#standalone-caching-service",
    "title": "Caching in DataFusion",
    "section": "Standalone caching service",
    "text": "Standalone caching service\nSo far we have discussed caching within individual DataFusion instances, as shown in the Figure below (upper). While this approach works well for single-instance deployments, it is wasteful when multiple DataFusion instances access the same data.\nA more scalable approach is to implement a standalone shared caching service (lower) that can be accessed by multiple DataFusion instances, with the following advantages:\n\nReduced resource usage - By eliminating redundant caching and decoding across instances, it optimizes both memory and CPU utilization\nImproved manageability - Decoupling the caching logic from compute nodes simplifies scaling and operational management\nConsistent performance - No latency spike after compute node restarts. With stateless compute nodes and a persistent centralized cache, the system avoids cache warmup delays after node restarts\n\n\n\nCaching interface\nThe simplest caching interface treats the caching service as a transparent proxy for object storage. The service implements DataFusion’s AsyncFileReader trait to intercept Parquet file reads, caching byte ranges as they are accessed.\nThe bytes are transferred between DataFusion and the cache service using HTTP and can be implemented in any programming languages. This simple interface requires minimal changes to DataFusion.\nA more sophisticated approach is to implement caching through the Arrow Flight protocol, with the caching service running as a full DataFusion instance capable of executing query plans. Rather than just serving cached bytes, this design allows the caching service to process queries directly.\nWhen a querier needs data, it sends the complete ParquetExec physical plan to the caching service. The service can then:\n\nExecute the plan against its cached data\nApply filters and projections directly on the cached data (filter pushdown)\nReturn only the necessary Arrow RecordBatches to the querier\n\nThis architecture provides several advantages: - Reduced network transfer by filtering data at the cache layer - Lower client-side CPU usage since filtering happens at the cache\nThe tradeoff is increased complexity in both the client and cache service implementations compared to the simple byte-range caching approach.\n\n\nCaching medium\nThe standalone caching service allows a diverse set of storage mediums to optimize the performance and cost.\nThe most obvious choice is to cache data in memory, which is fast but expensive. We can do better by spilling the data to SSD when memory is full, and to HDD when SSD is full."
  },
  {
    "objectID": "posts/caching-datafusion/index.html#conclusion",
    "href": "posts/caching-datafusion/index.html#conclusion",
    "title": "Caching in DataFusion",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we discussed DataFusion’s caching mechanisms and explored the potential of caching Arrow data in standalone caching services. We believe that caching is a key component for every cloud-native analytics systems, and my research project is actively building systems that bridges the gap between what academia known as the state-of-the-art and what industry can actually use."
  },
  {
    "objectID": "posts/caching-datafusion/index.html#footnotes",
    "href": "posts/caching-datafusion/index.html#footnotes",
    "title": "Caching in DataFusion",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBtrBlocks: Efficient Columnar Compression for Data Lakes (SIGMOD 2023 Paper)↩︎"
  },
  {
    "objectID": "posts/system-researchers/index.html",
    "href": "posts/system-researchers/index.html",
    "title": "Where are we now, system researchers?",
    "section": "",
    "text": "We, as system researchers, face an existential crisis. We find ourselves questioning our unique strengths and fundamental purpose where industry increasingly outpaces academic innovation."
  },
  {
    "objectID": "posts/system-researchers/index.html#research-is-a-privilege",
    "href": "posts/system-researchers/index.html#research-is-a-privilege",
    "title": "Where are we now, system researchers?",
    "section": "Research is a privilege",
    "text": "Research is a privilege\nResearch is paid for by taxpayers, but we often forget this is a gift. We think we deserve this money because we’re smart or have fancy degrees, without asking if we really earned it.\nI think spending money on research is good for everyone, but that doesn’t mean we can just take the money without being responsible to the public.\nWhen someone pull the plug on the research funding, we could easily blame the person, and name a few research achievements that would be impossible without those funding. But from the bottom of my heart, I often have the same question that whether some of the research funding would be better spent on something else.\nI’m not saying we should only fund research that promises clear results – that would go against the whole point of exploring new ideas. What worries me is research funding decisions are unaccountable to the public. The people who decide where money goes are usually academics themselves, who are often detached and highly alienated with what regular people actually need or care about. We often talk only to each other, praising work that looks good on paper but doesn’t help real people. Over time, with no one making us prove our worth, our research drifts further from what actually matters.\nBut the public has one last ultimate weapon — cutting off the money entirely. This tough move hurts, but sometimes it’s the only way to make researchers listen: we need to do work that actually matters to people.\nResearch is a privilege, not an entitlement. We must question whether our work truly justifies the public investment we receive and delivers meaningful value to society."
  },
  {
    "objectID": "posts/system-researchers/index.html#system-research-is-irrelevant",
    "href": "posts/system-researchers/index.html#system-research-is-irrelevant",
    "title": "Where are we now, system researchers?",
    "section": "System research is irrelevant",
    "text": "System research is irrelevant\nSystem research is irrelevant. Industry has become the better place for meaningful systems work. Most impactful and innovative systems today come from companies, not universities.\nIndustry has the money and patience to build complete systems. But most importantly, industry systems are accountable – systems that don’t deliver value get shut down quickly. This accountability creates a natural selection process. Industry systems must stay relevant or die. They evolve to meet real needs or disappear.\nDue to the unaccountable funding agencies mentioned above, research systems are often one-shot projects —- they are immediately abandoned right after publication. We are frequently impressed by the great systems coming from industry (the most recent example being the infrastructure at DeepSeek), and we’ve realized that we are far from competing with them. As a result, we seem to constrain ourselves to a few narrow research topics, solving problems that are difficult to connect with even a single real user (essentially imaginary problems). Or we simply adjust our goal from research to education, preparing students for their industry jobs, so they can continue research there. But if that is the case, why would we need research funding at all?"
  },
  {
    "objectID": "posts/system-researchers/index.html#we-are-unqualified",
    "href": "posts/system-researchers/index.html#we-are-unqualified",
    "title": "Where are we now, system researchers?",
    "section": "We are unqualified",
    "text": "We are unqualified\nPhD students are typically only a few years older than undergraduates. How can they possibly compete with senior industry practitioners who have been working on the same problems for decades?\nYet we are not even trying to compete. We don’t like “engineering problems”, because we researchers are supposed to work on “research problems”.\nWe like research problems for two simple reasons:\n\nFeeling “researchy” makes us feel good about ourselves; simply thinking about fancy terminology makes us feel fundamental and important.\nWe don’t know how to code.\n\nWe waste too much time babbling about knowledge we learn from papers – how to schedule a million machines, how to train a billion parameters, how to design infinitely scalable systems. Just thinking about these problems makes us feel important as researchers, although most of us have never deployed a service in the cloud, never used the techniques we proposed, and never worked with the filesystems, kernels, compilers, networks, or databases we studied. We waste time on these theoretical discussions because we don’t know how to code and are unwilling to practice. As Feynman said, “What I cannot create, I do not understand.” Simply knowing how a system works from 1000 feet doesn’t mean we can build it. The nuances of real systems often explain why they’re built in particular ways. Without diving into these details, we’re merely scratching the surface.\nHere are my bold claims:\n\nYour opinion doesn’t matter until you write &gt;50k lines of system code.\nYour novel idea is 💩 if it’s not implemented in a system with real users.\n\n(Writing code does not make you a good researcher, but not writing code makes you a bad one.)\nThe system research community does not need more novel solutions – novel solutions are essentially combinations of existing techniques. When we need to solve a problem, most of us would figure out a similar solution, and what matters is the execution of the ideas.\nInstead, we need more people willing to sit down and code, build real systems, and talk to real users. Be a solid practitioner, don’t be a feel-good researcher."
  },
  {
    "objectID": "posts/system-researchers/index.html#we-are-pushing-young-researchers-away",
    "href": "posts/system-researchers/index.html#we-are-pushing-young-researchers-away",
    "title": "Where are we now, system researchers?",
    "section": "We are pushing young researchers away",
    "text": "We are pushing young researchers away\nWE ARE TOO SLOW because we waste too much time on procedural overhead rather than actual scientific exploration.\nPaper publishing takes too much time. We spend too much effort arguing what’s new and what’s hard, instead of focusing on doing the actual research. Writing a paper already takes too much time, and then we need to anonymize artifacts, register abstracts, wait for reviews, write rebuttals, revise the paper, and can still be rejected for arbitrary reasons. The turnaround time for a single submission can be up to 6 months.\nThere is also a dark side to paper publishing. The entire publication process can be exploited by determined actors, and we are unwilling to address this issue, probably because many big names are involved. Young researchers see low-quality papers being accepted, yet their own carefully crafted systems are rejected for arbitrary reasons.\nAll of these burn our time and energy, pushing young researchers away from the community. After all, research doesn’t have to happen in academia."
  },
  {
    "objectID": "posts/system-researchers/index.html#our-measurement-is-wrong",
    "href": "posts/system-researchers/index.html#our-measurement-is-wrong",
    "title": "Where are we now, system researchers?",
    "section": "Our measurement is wrong",
    "text": "Our measurement is wrong\nAll of our research projects start with the two questions of what’s new and what’s hard, but unfortunately, these two questions have guided us toward irrelevancy. Many one-shot papers claim novelty and then disappear, preventing future research projects from making progress. They take credit for being the first to introduce an idea, even when the implementation doesn’t work or is completely wrong. Yet, all future researchers are required to compare their work against these papers.\nMost of the time, the code is terribly implemented or overly simplified, making fair comparisons impossible. But reviewers don’t care – they see the two papers as informationally equivalent, viewing the same idea from a 1000-foot perspective, and lazily question authors about what’s new and what’s hard.\nThe real difference between papers often lies in numerous small details that sound trivial but are actually essential for relevance. In most cases, figuring out these details takes much more time and demonstrates more novelty than coming up with the initial idea itself.\nThe reviewers – often just a few years older than PhD students, don’t know how to code, unable to delve into implementation details, and incapable of appreciating real system nuances – will likely reject the paper for lack of novelty."
  },
  {
    "objectID": "posts/system-researchers/index.html#system-research-is-knowing",
    "href": "posts/system-researchers/index.html#system-research-is-knowing",
    "title": "Where are we now, system researchers?",
    "section": "System research is knowing",
    "text": "System research is knowing\n\nDespite all previous arguments, I still believe there is value in systems research, and I still believe we can be relevant, but that’s for another post.\nSystem research is about knowing – understanding systems deeply and completely, contrasting with AI research that embraces uncertainty and probabilistic approaches.\nWe study the complex interactions of systems, and practice to know them better. We fight against the seemingly infinite complexity of systems, and uncover the underlying principles.\nKnowing requires a lot of practicing – hard work, patience, and a lot of coding – and we need a culture that cultivates this."
  },
  {
    "objectID": "posts/parquet-viewer/index.html",
    "href": "posts/parquet-viewer/index.html",
    "title": "Online Parquet Viewer with Rust",
    "section": "",
    "text": "Online here: https://parquet-viewer.xiangpeng.systems\nSource code: parquet-viewer\n\nRun SQL queries \nVisualize query plans \nExamine metadata"
  },
  {
    "objectID": "posts/parquet-viewer/index.html#quick-look",
    "href": "posts/parquet-viewer/index.html#quick-look",
    "title": "Online Parquet Viewer with Rust",
    "section": "",
    "text": "Online here: https://parquet-viewer.xiangpeng.systems\nSource code: parquet-viewer\n\nRun SQL queries \nVisualize query plans \nExamine metadata"
  },
  {
    "objectID": "posts/parquet-viewer/index.html#how-it-works",
    "href": "posts/parquet-viewer/index.html#how-it-works",
    "title": "Online Parquet Viewer with Rust",
    "section": "How it works",
    "text": "How it works\nIt compiles Parquet/Arrow/DataFusion to webassembly which runs in the browser.\nSpecifically, it uses trunk to pack wasm files, leptos to build reactive UI components, and Tailwind CSS for styling.\nAnd most importantly, LLM wrote most of the code."
  },
  {
    "objectID": "posts/parquet-viewer/index.html#why-do-i-need-this",
    "href": "posts/parquet-viewer/index.html#why-do-i-need-this",
    "title": "Online Parquet Viewer with Rust",
    "section": "Why do I need this?",
    "text": "Why do I need this?\nParquet files are not human-readable, you can’t just open them like CSV or JSON files.\nOf course, there are many CLI tools that allow you to inspect file content – if you enjoy deciphering cryptic CLI args.\nThere are also Java/C#/Windows applications that provide GUI options – if you’re nostalgia for the 90s.\nBut with this tool, everything happens inside your favorite browser, written in your favorite language, using your favorite tech stack.\nSimply drag and drop the file, and all the important information is in front of you."
  },
  {
    "objectID": "posts/parquet-viewer/index.html#rust-for-frontend",
    "href": "posts/parquet-viewer/index.html#rust-for-frontend",
    "title": "Online Parquet Viewer with Rust",
    "section": "Rust for frontend?",
    "text": "Rust for frontend?\n\nThe good\n\nLeptos is surprisingly intuitive and easy-to-use.\nDataFusion/Arrow/Parquet compiles to wasm with almost no modification.\nThere’s surprisingly small amount of code needed to build an interactive UI.\n\n\n\nThe bad\n\nBuild size is large, debug build is 100MB, release build is 40MB. Poor dev experience due to laggy edit-build-run cycle. (User experience is fine, though)\nNot every crate is wasm-compatible, and it’s very difficult to triangle down the root cause.\nI haven’t figure out how to nicely use JavaScript yet – sometimes you do need JS."
  },
  {
    "objectID": "posts/stop-building-agent-systems/index.html",
    "href": "posts/stop-building-agent-systems/index.html",
    "title": "Stop building systems for agents",
    "section": "",
    "text": "The Wrong Direction\nEveryone knows today’s systems are sub-optimal for agents. They waste context, confuse models with unstructured output, and prioritize vision-based human interaction over text-based LLMs.\nSo we desperately need to build agent systems that treat LLMs as first-class citizens. I’m sure you can generate a hundred papers or spawn a hundred startups in this direction.\nThis approach surely works, but it is not fundamental: if humans can live with it, LLMs can eventually catch up. Maybe next month, maybe next year, but eventually LLMs can easily do what humans can do, and then your mission-critical agent-native system becomes a nice-to-have.\nIn the LLM era, we need to ask not what humans can do for LLMs, but what LLMs can do for humans.\nSo what is the fundamental challenge here? The velocity at which a human can take responsibility for an agent’s actions.\n\n\nThe Unaccountable Machine\n\nIn human society, every functional system relies on an accountability chain. When things break, we need a person to blame, and to fix it.\nFor the last few decades, we maintained a subtle balance where the velocity of building systems roughly equals the velocity of accountability.\nLLMs changed this by making it 1000x faster to write code, but our ability to hold accountability for a system barely changed.\nThis creates a perfect accountability sink. When a vibe-system fails, we can only hope LLMs will fix it by themselves. But systems that are unaccountable are useless, and will eventually collapse.\nMost LLM-vibed systems look fancy at first, but without a human owning their behavior, they are useless at best, and burdensome to society at worst.\nAxiom: AI systems, no matter how capable, must be held accountable by a human.\nTherefore, the real bottleneck of agent adoption is the Time to Accountability: how quickly can a human operator understand, diagnose, and own the system’s behavior?\n\n\nThe “Happen-to-Work” Foundation\n(Un)Surprisingly, this bottleneck has almost nothing to do with AI.\nYou join a new team. How long does it take for you to be held accountable for a production system? It happens when you can observe the system interactions, reproduce regressions and failures, and explain what’s going on.\nYou may think, isn’t this what we have been preparing for? Hasn’t computer science been working on this for decades? Isn’t this a solved problem?\nUnfortunately, the answer is no. We don’t have enough infrastructure to understand the systems we build, maintain, and rely on.\nWhen we compile software, it just happens-to-compile. We hope we are using the right compiler version, compiling the right dependencies, executing in the correct parallel order, and persisting files without corrupting data (in fact, we don’t even know if the data is corrupted or persisted).\nWhen we issue a database transaction, it just happens-to-work. We hope the data transferred from disk returns in a deterministic order, that transactions respect isolation levels, and that our access controls actually work.\n\n\nLLM Agents Shake Our Fragile Foundation\nHow could it be that our modern society is based on something so fragile that nobody understands? You might ask.\nThe answer is simple: we only spent minimal effort to make it work most of the time. When rare edge cases happen, we just retry and hope they don’t appear. No one is able to reproduce a rare production bug, no one is able to fully explain why performance jitters, and no one is able to fully understand how systems interact.\nIt has been fine, you may argue. But no, it’s not fine. As LLMs elevate humanity to the next level, what used to be ok is no longer ok.\nWe’re now capable of operating tremendously more systems; tremendous amounts of work will be cheaply digitalized, and a tremendous number of tasks will be cheaply performed by LLMs. When the scale grows 100 times, our fragile foundation becomes the problem.\n\n\nThe Real Agent-Native Systems\nSo what defines an agent-native system? They are human-centric:\n\nRadically observable.\nRadically deterministic.\n\nRadically observable means we must see exactly how the system performs: who invokes a system call, how threads interleave, which function writes to a memory address, etc. We should act like a glass box—observing system behavior from all levels of the stack. If performance regresses, we should see the exact reason, whether the CPU core is overheating or the SSD firmware is garbage collecting.\nRadically deterministic means that given the same input, the program produces exactly the same output (randomness is controlled via seeds). This applies not only to a single function, but to the entire multi-threaded, distributed system. Threads must interleave in exactly the same way, hash tables must iterate in exactly the same order, and environmental noise (like noisy neighbors) must never alter the software’s execution.\nStop building systems tailored for agents, build systems that easier for human to own, make them observable and deterministic."
  }
]